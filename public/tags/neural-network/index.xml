<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Neural Network on Young Story</title><link>https://yyq.github.io/tags/neural-network/</link><description>Recent content in Neural Network on Young Story</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 05 Aug 2017 00:00:00 +0000</lastBuildDate><atom:link href="https://yyq.github.io/tags/neural-network/index.xml" rel="self" type="application/rss+xml"/><item><title>coursera Andrew Ng 机器学习第五周笔记 神经网络的学习过程</title><link>https://yyq.github.io/posts/2017/2017-08-05-machine-learning-week5/</link><pubDate>Sat, 05 Aug 2017 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2017/2017-08-05-machine-learning-week5/</guid><description>Neural Networks: Learning cost function L = total number of layers in the network sl = number of units (not counting bias unit) in layer l K = number of output units/classes the double sum simply adds up the logistic regression costs calculated for each cell in the output layer the triple sum simply adds up the squares of all the individual Θs in the entire network. the i in the triple sum does not refer to training example i Backpropagation Algorithm Backpropagation in Practice thetaVector = [ Theta1(:); Theta2(:); Theta3(:); ] deltaVector = [ D1(:); D2(:); D3(:) ] Theta1 = reshape(thetaVector(1:110),10,11) Theta2 = reshape(thetaVector(111:220),10,11) Theta3 = reshape(thetaVector(221:231),1,11) gradient checking Once you have verified once that your backpropagation algorithm is correct, you don&amp;rsquo;t need to compute gradApprox again.</description></item></channel></rss>