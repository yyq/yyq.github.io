<!doctype html><html lang=en-us>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge,chrome=1">
<title>ai | Young Story</title>
<meta name=viewport content="width=device-width,minimum-scale=1">
<meta name=description content>
<meta name=generator content="Hugo 0.91.2">
<meta name=ROBOTS content="NOINDEX, NOFOLLOW">
<link rel=stylesheet href=/ananke/css/main.min.css>
<link href=/tags/ai/index.xml rel=alternate type=application/rss+xml title="Young Story">
<link href=/tags/ai/index.xml rel=feed type=application/rss+xml title="Young Story">
<meta property="og:title" content="ai">
<meta property="og:description" content>
<meta property="og:type" content="website">
<meta property="og:url" content="https://yyq.github.io/tags/ai/">
<meta itemprop=name content="ai">
<meta itemprop=description content><meta name=twitter:card content="summary">
<meta name=twitter:title content="ai">
<meta name=twitter:description content>
</head>
<body class="ma0 avenir bg-near-white">
<header>
<div class="pb3-m pb6-l bg-black">
<nav class="pv3 ph3 ph4-ns" role=navigation>
<div class="flex-l justify-between items-center center">
<a href=/ class="f3 fw2 hover-white no-underline white-90 dib">
Young Story
</a>
<div class="flex-l items-center">
<div class=ananke-socials>
</div>
</div>
</div>
</nav>
<div class="tc-l pv3 ph3 ph4-ns">
<h1 class="f2 f-subheadline-l fw2 light-silver mb0 lh-title">
ai
</h1>
</div>
</div>
</header>
<main class=pb7 role=main>
<article class="cf pa3 pa4-m pa4-l">
<div class="measure-wide-l center f4 lh-copy nested-copy-line-height nested-links nested-img mid-gray">
<p>Below you will find pages that utilize the taxonomy term “ai”</p>
</div>
</article>
<div class="mw8 center">
<section class="flex-ns flex-wrap justify-around mt5">
<div class="relative w-100 mb4 bg-white">
<div class="relative w-100 mb4 bg-white nested-copy-line-height">
<div class="bg-white mb3 pa4 gray overflow-hidden">
<span class="f6 db">Posts</span>
<h1 class="f3 near-black">
<a href=/posts/2017/2017-08-12-machine-learning-exercise/ class="link black dim">
coursera Andrew Ng 机器学习 编程习题总结
</a>
</h1>
<div class="nested-links f5 lh-copy nested-copy-line-height">
学习这些知识不是为了学习而学习，是为了解决问题而学习。所以这些知识所配套的编程练习尤为重要了。这门课程做的作业，可算是让我开了眼界，简单的一些代码和并不是特别大量的数据，就能解决一些简单的问题。受益匪浅！记录下来这些，以供之后解决复杂问题的时候给一个参考思路。
这门课程的代码我是用的matlab，我的代码参考链接点击这里
Linear Regression 解决预测房价的问题，已知面积，房间数 写出线性回归的代价函数和梯度下降函数 画图来看，有个直观感受 不同的learning rate有不同的影响，收敛快慢 Logistic Regression 预测某个学生是否通过了某门课程，已知数据包括两次测验的成绩，画出直线决策边界
预测制造工厂出产的芯片的质量合格与否，根据芯片的两次测试成绩，画出像圆形的决策边界
sigmoid函数
代价函数，梯度下降函数
范化
Multi-class classification and Neural Network 数字图片的识别，输入图片，输出数字 通过1-vs-N的逻辑回归函数来识别 通过神经网络来识别(这里只实现了前向反馈用来预测)，theta矩阵已知，效果显然比逻辑回归好 Neural Networks Learning 仍然是识别数字 实现了神经网络里的后向反馈，代价函数，梯度下降，随机初始化，泛化，梯度检查等步骤 可视化隐藏层，有个直观感受 Regularized Linear Regression and Bias v.s. Variance 不同的偏差，方差，通过画图观察 画学习曲线，发现了高偏差问题，数据量增多也不管用，那么线性改多项式 多项式回归，画图，调整选择lambda Support Vector Machines 识别垃圾邮件，亲测自己收到的各种广告垃圾邮件，果然有效！ Informally, the C parameter is a positive value that controls the penalty for misclassified training examples SVM with 高斯内核 决策边界，无比扭曲的曲线都可以适应好 垃圾邮件case:邮件单词预处理，词库预处理，提取邮件特征，训练，预测，GoodJob!
</div>
</div>
</div>
</div>
<div class="relative w-100 mb4 bg-white">
<div class="relative w-100 mb4 bg-white nested-copy-line-height">
<div class="bg-white mb3 pa4 gray overflow-hidden">
<span class="f6 db">Posts</span>
<h1 class="f3 near-black">
<a href=/posts/2017/2017-08-11-machine-learning-week11/ class="link black dim">
coursera Andrew Ng 机器学习第十一周笔记 应用举例：photo OCR
</a>
</h1>
<div class="nested-links f5 lh-copy nested-copy-line-height">
Application example: photo OCR pipeline text detection character segmentation character classification sliding windows too easy to learn Getting more data: artificial data synthesis Discussion on getting more data
make sure you have a low bias classifier before expending the effort.(Plot learning curves.) keep increasing the nuber of features/number of hidden units in neural network until you have a low bias classifier how much work would it be to get 10x as much data as we currently have?
</div>
</div>
</div>
</div>
<div class="relative w-100 mb4 bg-white">
<div class="relative w-100 mb4 bg-white nested-copy-line-height">
<div class="bg-white mb3 pa4 gray overflow-hidden">
<span class="f6 db">Posts</span>
<h1 class="f3 near-black">
<a href=/posts/2017/2017-08-10-machine-learning-week10/ class="link black dim">
coursera Andrew Ng 机器学习第十周笔记 大数据量的机器学习
</a>
</h1>
<div class="nested-links f5 lh-copy nested-copy-line-height">
stochastic gradident descent mini-batch gradient descent Batch gradient descent: use all m examples in each iteration
Stochastic gradient descent: use 1 examples in each iteration
Mini-batch gradient descent: use b examples in each iteration
checking for convergence every 1000 iterations, plot cost(theta, xi, yi) averaged over the last 1000 examples processed by algorithm.
online learning example shiping service website where user comes, specifies origin and destination, you offer to ship their package for some asking price, and user sometimes choose to use your shipping service(y=1), sometimes not(y=0).
</div>
</div>
</div>
</div>
<div class="relative w-100 mb4 bg-white">
<div class="relative w-100 mb4 bg-white nested-copy-line-height">
<div class="bg-white mb3 pa4 gray overflow-hidden">
<span class="f6 db">Posts</span>
<h1 class="f3 near-black">
<a href=/posts/2017/2017-08-09-machine-learning-week9/ class="link black dim">
coursera Andrew Ng 机器学习第九周笔记 异常检测与推荐系统
</a>
</h1>
<div class="nested-links f5 lh-copy nested-copy-line-height">
Anomaly Detection Density estimation Dataset: x1, x2, x3, &mldr; xm Is X-test anomalous ?
algorithm anomaly detection vs supervised learning choosing what features to use non-gaussian features, maybe log(x),sqrt(x),x^2 to gaussian-like features
multivariate G distributon Don&rsquo;t model p(x1) p(x2) separately.
Modle p(x) all in one go.
Recommender Systems collaborative filtering algorithm finding related movies smallest|| xi - xj ||
users who have not rated any movies mean normalization
</div>
</div>
</div>
</div>
<div class="relative w-100 mb4 bg-white">
<div class="relative w-100 mb4 bg-white nested-copy-line-height">
<div class="bg-white mb3 pa4 gray overflow-hidden">
<span class="f6 db">Posts</span>
<h1 class="f3 near-black">
<a href=/posts/2017/2017-08-08-machine-learning-week8/ class="link black dim">
coursera Andrew Ng 机器学习第八周笔记 降低维度
</a>
</h1>
<div class="nested-links f5 lh-copy nested-copy-line-height">
unsupervised learning k-means altorithm clustering, optimization objective clustering, random initialization should have K &lt; m
Randomly pick K training examples.
clustering, choosing the number of clusters, K Please draw a graph. Elbow method
Dimensionality reduction Motivation I: data compressioni
Motivation II: Visualization
Princiapl Component Analysis reduce from n-dimension to k-dimension: find k vectors u1 u2 u3 uk onto which to project the data,so as to minimize the projection error.
PCA steps
</div>
</div>
</div>
</div>
<div class="relative w-100 mb4 bg-white">
<div class="relative w-100 mb4 bg-white nested-copy-line-height">
<div class="bg-white mb3 pa4 gray overflow-hidden">
<span class="f6 db">Posts</span>
<h1 class="f3 near-black">
<a href=/posts/2017/2017-08-07-machine-learning-week7/ class="link black dim">
coursera Andrew Ng 机器学习第七周笔记 支持向量机
</a>
</h1>
<div class="nested-links f5 lh-copy nested-copy-line-height">
Support Vector Machines optimization Objective Large Margin Intuition Kernels Using an SVM choice of parameter C
choice of kernel(similarity function)
if Gaussian Kernel, need to choose sigma^2
Not all similarity functions make valid kernels(need to satisfy technical condition called &ldquo;Mercer&rsquo;s Theorem&rdquo; to make sure SVM packages' optimizations run correctly, and do not diverge).
Polynomial kernel: More esoteric: String kernel, chi-square kernel, histogram intersection kernel
multi-class calssification
K SVMs, one to distinguish one from the rest.
</div>
</div>
</div>
</div>
<div class="relative w-100 mb4 bg-white">
<div class="relative w-100 mb4 bg-white nested-copy-line-height">
<div class="bg-white mb3 pa4 gray overflow-hidden">
<span class="f6 db">Posts</span>
<h1 class="f3 near-black">
<a href=/posts/2017/2017-08-06-machine-learning-week6/ class="link black dim">
coursera Andrew Ng 机器学习第六周笔记 系统设计与优化技巧
</a>
</h1>
<div class="nested-links f5 lh-copy nested-copy-line-height">
Advice for Applying Machine Learning 重要的笔记，必须手写上传 Machine Learning System Design email spam example Given a data set of emails, we could construct a vector for each email. Each entry in this vector represents a word. The vector normally contains 10,000 to 50,000 entries gathered by finding the most frequently used words in our data set. If a word is to be found in the email, we would assign its respective entry a 1, else if it is not found, that entry would be a 0.
</div>
</div>
</div>
</div>
<div class="relative w-100 mb4 bg-white">
<div class="relative w-100 mb4 bg-white nested-copy-line-height">
<div class="bg-white mb3 pa4 gray overflow-hidden">
<span class="f6 db">Posts</span>
<h1 class="f3 near-black">
<a href=/posts/2017/2017-08-05-machine-learning-week5/ class="link black dim">
coursera Andrew Ng 机器学习第五周笔记 神经网络的学习过程
</a>
</h1>
<div class="nested-links f5 lh-copy nested-copy-line-height">
Neural Networks: Learning cost function L = total number of layers in the network sl = number of units (not counting bias unit) in layer l K = number of output units/classes the double sum simply adds up the logistic regression costs calculated for each cell in the output layer the triple sum simply adds up the squares of all the individual Θs in the entire network. the i in the triple sum does not refer to training example i Backpropagation Algorithm Backpropagation in Practice thetaVector = [ Theta1(:); Theta2(:); Theta3(:); ] deltaVector = [ D1(:); D2(:); D3(:) ] Theta1 = reshape(thetaVector(1:110),10,11) Theta2 = reshape(thetaVector(111:220),10,11) Theta3 = reshape(thetaVector(221:231),1,11) gradient checking Once you have verified once that your backpropagation algorithm is correct, you don&rsquo;t need to compute gradApprox again.
</div>
</div>
</div>
</div>
<div class="relative w-100 mb4 bg-white">
<div class="relative w-100 mb4 bg-white nested-copy-line-height">
<div class="bg-white mb3 pa4 gray overflow-hidden">
<span class="f6 db">Posts</span>
<h1 class="f3 near-black">
<a href=/posts/2017/2017-08-04-machine-learning-week4/ class="link black dim">
coursera Andrew Ng 机器学习第四周笔记 神经网络模型
</a>
</h1>
<div class="nested-links f5 lh-copy nested-copy-line-height">
Neural Networks:Representation 这个周的课程很简单，就是介绍了神经网络是什么样子的
introduction examples multi classification example
</div>
</div>
</div>
</div>
<div class="relative w-100 mb4 bg-white">
<div class="relative w-100 mb4 bg-white nested-copy-line-height">
<div class="bg-white mb3 pa4 gray overflow-hidden">
<span class="f6 db">Posts</span>
<h1 class="f3 near-black">
<a href=/posts/2017/2017-08-03-machine-learning-week3/ class="link black dim">
coursera Andrew Ng 机器学习第三周笔记 逻辑回归与泛化
</a>
</h1>
<div class="nested-links f5 lh-copy nested-copy-line-height">
Logistic Regression Classification The classification problem is just like the regression problem, except that the values y we now want to predict take on only a small number of discrete values. For now, we will focus on the binary classification problem in which y can take on only two values, 0 and 1.
Logistic Regression Model Decision Boundary The decision boundary is the line that separates the area where y = 0 and where y = 1.
</div>
</div>
</div>
</div>
<div class="relative w-100 mb4 bg-white">
<div class="relative w-100 mb4 bg-white nested-copy-line-height">
<div class="bg-white mb3 pa4 gray overflow-hidden">
<span class="f6 db">Posts</span>
<h1 class="f3 near-black">
<a href=/posts/2017/2017-08-02-machine-learning-week2/ class="link black dim">
coursera Andrew Ng 机器学习第二周笔记 多个变量的线性回归
</a>
</h1>
<div class="nested-links f5 lh-copy nested-copy-line-height">
Linear Regression with Multiple Variables Multiple Features Gradient Descent for Multiple Variables Feature Scaling We can speed up gradient descent by having each of our input values in roughly the same range.
feature scaling: involves dividing the input values by the range of the input variable resulting in a new range of just 1. mean normalization: involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero.
</div>
</div>
</div>
</div>
<div class="relative w-100 mb4 bg-white">
<div class="relative w-100 mb4 bg-white nested-copy-line-height">
<div class="bg-white mb3 pa4 gray overflow-hidden">
<span class="f6 db">Posts</span>
<h1 class="f3 near-black">
<a href=/posts/2017/2017-08-01-machine-learning-week1/ class="link black dim">
coursera Andrew Ng 机器学习第一周笔记 线性回归
</a>
</h1>
<div class="nested-links f5 lh-copy nested-copy-line-height">
machine learning supervised learning, unsupervised learning, others(reinforcement learning, recommender system)
supervised learning regressing，predict input into continuous result classification, predict input into distract result examples：
given size, rooms, predict house price, like 1M, 100Grand, 1B house prize is expensive or cheap ? given picture with human, predict his age: 0.1，12.5，78.8 given a patient with tumor, predict it&rsquo;s a malignant or benign unsupervised learning problems with little or no idea what our results should look like
</div>
</div>
</div>
</div>
</section>
</div>
</main>
<footer class="bg-black bottom-0 w-100 pa3" role=contentinfo>
<div class="flex justify-between">
<a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://yyq.github.io/>
&copy; Young Story 2022
</a>
<div>
<div class=ananke-socials>
</div></div>
</div>
</footer>
</body>
</html>