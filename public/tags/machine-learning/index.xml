<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>machine learning on Young Story</title><link>https://yyq.github.io/tags/machine-learning/</link><description>Recent content in machine learning on Young Story</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 22 Oct 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://yyq.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>对数据科学实验的思考</title><link>https://yyq.github.io/posts/2020/2020-10-22-exp-thinking/</link><pubDate>Thu, 22 Oct 2020 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2020/2020-10-22-exp-thinking/</guid><description>从本周的一位同学的分享开始
他分享的是一系列特征有关的实验，末尾是QA环节，实验质量（实验方式，实验结果的分析）被多个角度质疑，显而易见，他的实验结果大家很难认可了。无论是他个人的时间花费，还是这次分享的质量，都有些苍白。
最近某项目小组，在一个调优目标上，多位同学参与，短时间内也做了大量的实验，从结果来看，无一显著性的提升。其实我的内心也有在怀疑小伙伴们实验做的很仓促（所以很粗糙），写的代码有bug么？上传到评分版的数据有传错么？实验结论难道仅仅只看最终的一个准确率指标比大小？有没有看稍微更详细的一点的实验结果？新加的改动，对重要的预测条目有什么影响？有的实验结果是准确率上升了，按理说应该很高兴，但是我也会问一下有从哪几个角度看一下数据泄露的可能性没？
实验结果最好不要只有最后一个准确率的数字，应该从一个准确率的数字，下钻到更多的细节，有更多的观察，然后给出进一步的分析结论，进而给出进一步想探索的实验思路。
实验的分析链条上，尽量的严密，逻辑要清晰，连贯，自洽。不要随意猜。
分析思路枯竭了，再开始猜，可以先有猜想，然后看数字来证明到自己的猜想是对还是错。
大家在学生阶段的课上应该都学过基本的实验思路，在动手开始实验之前，要明确实验的变量是什么，哪些是不变的，实验过程你会怎么做，预期实验结果是什么。如果结果和预期一致，那么说明了什么，证明了你的什么想法。如果结果和预期不一致，也请回顾实验过程的各个环节（有没有做错哪里？），每个环节的现象是什么，来间接推理为什么结果和预期不一致。想来，做数据科学的实验，也应该保持小时候做科学实验的初心，好奇，探索，严谨，而不是只交出一个数字。
之前和同事A交流的时候，讨论过需要收集很多很多实验结果，来做meta learning，应是有趣的一条路；目前，我的想法有变，单次实验的质量比大量（不一定准确的）实验结果更加重要。
为了尽量去保证团队中各位同学的实验质量，可以有什么手段呢？手把手指导，言传身教？我也没想清楚。
以上的思考，或许是这此听分享给我带来的最大的价值。</description></item><item><title>kaggle | quickdraw-doodle-recognition top 6%</title><link>https://yyq.github.io/posts/2018/2018-12-07-doodle-recognition/</link><pubDate>Fri, 07 Dec 2018 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2018/2018-12-07-doodle-recognition/</guid><description>前言 这次是google举办的一个“你画我猜”的题目，题目链接：kaggle - quickdraw-doodle-recognition。给的数据是游戏玩家在屏幕上画画的每一笔的坐标起始位置，然后让训练模型来猜出这幅画是画的什么，4900万张画，340个category。我记得微信上有个小游戏就是这么个东东。
本来没打算做这个比赛的，家里事情多了一些，然后毕竟cv也不是自己擅长的，后来吧，觉着GCP上还有1500多刀的过期时间和这个比赛时间一样，不用白不用，那就突破一下自己的comfort zone。比赛前期一直是自己在刷，最后8天的时候，自己做的两个单模在0.932，然后有找cv方面比较擅长的朋友组队，最后成绩还是略遗憾，没拿到银牌，和实力有关，也和运气有关吧，最后选交的不是队伍的最高分数。队友还是有比较伤心的，同时感觉自己年龄大了，对这个成绩和分数不是那么看重了，在论坛浏览了很多，和队友讨论了很多，过程中自己实践了很多，这才是对我最有价值的体验。
自己的收获 总的来说，认真从头到尾实践了cv的比赛，谢谢google送我的free credits，大幅修炼了我多显卡的实践能力（双卡？四卡？nonono，小伙子同时使用8个v100了解一下，还为了训练速度认真调优了显卡，cpu的使用率和io的效率等）。
图像有关的题嘛，卷积神经网络是肯定得用上了，然后笔画顺序有关嘛，循环神经网络肯定能有贡献，最后自己训练的两个单模se-resnet50和xception都是CNN类的，双向的lstm玩崩了，试了若干次都是第一个epoch顺利完成第2个epoch中间崩掉，至今没找到错误原因在哪。
下面总结一下自己的收获和踩过的坑吧：
先看来自蛙神的建议：bigger img size matters, larger batch_size matters, training number matters, more iterations matters, 简单验证过他这几个idea，全对！但是，卧槽了，这么些牛逼的建议再加上4900万张图，云主机党表示钱包好慌张 最新版的TF里自带的keras版本号是2.1.6，多GPU有神坑，性能奇差无比，别问我怎么知道的，花了老子好多钱之后才知道！后来单独用的最新版2.2.4的keras有很大提升。 2.2.4的keras的multiGPU的model也有坑，save_model的时候，得找到多GPU的model里的那个单GPU model的那一层，然后存下来 keras的fit_generator之前用过，这次很熟悉的再来一次 json.loads比ast.literal_eval快了10倍不止 制作img的速度，cv2和pillow这两个库不相上下，其他的都是垃圾 做3-channel彩色图的结果比灰度图的结果更好 做图的时候，line width matters，我猜是太细的笔画pool了几次之后就损失了细节 pretrained并没有比随机初始化的好，至少我自己实验是这样子的，论坛上意见不一 图片数量太多，极其费时，用二分法查找到自己最大的batch_size 再次看到蛙神的帖子，感谢paperhttps://arxiv.org/pdf/1810.00736.pdf ,这篇里有一张图展示了各类state of the art models的性能对比吧，主要是top-1 accuracy, top-5 accuracy, Operations(G-FLOPs)，这对资源吃紧而又想要分高的朋友特别重要了 16 cpu core的电脑，训练时别把16个core都用完了，还得留几个给处理多GPUmodel，10个workers比较合适 以前只知道在若干个epoch之后调整学习率，这一个epoch至少20个小时的训练肯定不能这么玩，习得新的调整LR的技法，比如Cyclical Learning Rate 或者是余弦学习率衰减, 原来keras的callback还有个默认的操作手法是在batch_end时有所动作，论坛上还有其他调整LR的算法，值得慢慢把玩 实在是时间有限，精力有限，以后参赛想要高分，对于我这类水平的人说，尽早开始多试错才能弥补专业知识的不足 观摩大佬分享 第一名 [ods.ai]Pablos link CNN方面，尝试了这么多的模型: resnet18, resnet34, resnet50, resnet101, resnet152, resnext50, resnext101, densenet121, densenet201, vgg11, pnasnet, incresnet, polynet, nasnetmobile, senet154, seresnet50, seresnext50, seresnext101，1 channel的3 channel的图像，112的256的图像都试了，一共搞了40来个模型 (大佬是有多少机器资源？或者收敛速度是我的几十倍？)</description></item><item><title>kaggle | Machine Learning for Insights Challenge</title><link>https://yyq.github.io/posts/2018/2018-09-25-kaggle-model-insights/</link><pubDate>Tue, 25 Sep 2018 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2018/2018-09-25-kaggle-model-insights/</guid><description>前言 最近比较忙，这次不是比赛分享了，是来分享一个“挑战”，kaggle上用词说是Challenge，在我理解这个是某专家管理员建立了一个简短的course，介绍了某主题的玩法，然后让大家做做简单的练习，一起讨论交流一下。
这次学习的是一个关于模型洞察力的主题，原文链接https://www.kaggle.com/ml-for-insights-signup， 本文这里主要是将我看到的有用的内容按照自己的理解翻译，记录下来。这个ML for insights，字面直译为模型洞察力了，在我理解来则是模型可解释性分析。
My Insights 我个人在玩kaggle和工作中用到最多的主要是树类模型(lgb,xgb)和神经网络(cnn, rnn)，确实很少去思考其中的含义和解释性，如果让我自己回答这个问题，我理解到用决策树的信息熵计算统计概率得出叶子节点的重要性，再加上拟合残差的思路就是xgb类的算法了。而神经网络方面，我则简单的理解为求导拟合。高中课本里的一次函数二次函数的求导大家都会，神经网络只是用链式法则给若干个矩阵求导罢了，思路还是朝着目标去拟合。
如此之粗浅，见笑了，下面看看别人的insights玩法。
User Case 一个模型的哪些东西是可解释的 许多人认为机器学习模型是黑盒子，在他们认为模型可以做出很好的预测，但是大家无法理解这些预测背后的逻辑。确实是，很多数据科学家不知道如何用模型来解释数据的实际意义。这里的文章将会从这么几个方面来讨论：
哪些特征在模型看到是最重要的？ 关于某一条记录的预测，每一个特征是如何影响到最终的预测结果的？ 从大量的记录整体来考虑，每一个特征如何影响模型的预测的？ 为什么这些解释信息是有价值的 调试模型用 指导工程师做特征工程 指导数据采集的方向 指导人们做决策 建立模型和人之间的信任 调试模型用 一般的真实业务场景会有很多不可信赖的，没有组织好的脏数据。你在预处理数据时就有可能加进来了潜在的错误，或者不小心泄露了预测目标的信息等，考虑各种潜在的灾难性后果，debug的思路就尤其重要了。当你遇到了用现有业务知识无法解释的数据的时候，了解模型预测的模式，可以帮助你快速定位问题，balabala
指导特征工程 特征工程通常是提升模型准确率最有效的方法。特征工程通常涉及到到反复的操作原始数据(或者之前的简单特征)，用不同的方法来得到新的特征。
有时候你完成FE的过程只用到了自己的直觉。这其实还不够，当你有上百个原始特征的时候，或者当你缺乏业务背景知识的时候，你将会需要更多的指导方向。
这个预测贷款结果的kaggle竞赛就是一个比较极端的例子，这个比赛有上百个原始特征。并且因为隐私原因，特征的名称都是f1, f2, f3等等而不是普通的英文单词来描述。这就模拟了一个场景，你没有任何业务方面直觉的场景。有一位参赛者发现了某两个特征相减f527, f528可以创建出特别有用的新特征。拥有这个新特征的模型比没有这个特征的模型优秀很多。但是当你面对几百个特征时，你如何创造出这样优秀的特征呢？
在这门课程里，你将会学习到找到最重要的特征的方法，并且可以发现两个特别相关的特征，当面对越来越多的特征的时候，这种方法就会很重要啦。
指导未来数据收集 对于网上下载的数据集你完全控制不了。不过很多公司和机构用数据科学来指导他们从更多方面收集数据。一般来说，收集新数据很可能花费比较高或者不是很容易，所以大家很想要知道哪些数据是值得收集的。基于模型的洞察力分析可以教你很好的理解已有的特征，这将会帮助你推断什么样子的新特征是有用的
指导人们决策 一些决策是模型自动做出来的，虽然亚马逊不会用人工来决定展示给你网页上的商品，但是很多重要的决策是由人来做出的，而对于这些决定，模型的洞察力会比模型的预测结果更有价值。
建立信任 很多人在做重要决策的时候不会轻易的相信模型，除非他们验证过模型的一些基本特性，这当然是合理的。实际上，把模型的可解释性展示出来，如果可以匹配上人们对问题的理解，那么这将会建立起大家对模型的信任，即使是在那些没有数据科学知识的人群中。
Permuation Importance 一个最基本的问题大概会是什么特征对我模型预测的影响最大呢？ 这个东西就叫做“feature importance”即特征重要性。anyway，字面意思看这个东东就很重要啦。我们有很多方法来衡量特征的重要性，这里呢，将会介绍一种方法：排列重要性。这种方法和其他方法比起来，优势有：
计算速度快 广泛使用和理解 Consistent with properties we would want a feature importance measure to have 工作原理 排列重要性，一定是在model训练完成后，才可以计算的。简单来说，就是改变数据表格中某一列的数据的排列，看其对预测准确性的影响有多大。大概三个步骤：
训练好模型 拿某一个feature column, 然后随机打乱顺序。然后用模型来重新预测一遍，看看自己的metric或者loss function变化了多少 把上一个步骤中打乱的column复原，换下一个column重复上一个步骤，直到所有column都算一遍 代码示例 这个case是利用FIFA 18很多场的足球比赛的数据统计，来预测&amp;quot;Winner of The Game&amp;quot;</description></item><item><title>kaggle | avito-demand-prediction top 10%</title><link>https://yyq.github.io/posts/2018/2018-06-29-kaggle-avito/</link><pubDate>Fri, 29 Jun 2018 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2018/2018-06-29-kaggle-avito/</guid><description>题目链接:kaggle, avito是一家俄罗斯公司，从网站上来看是一个线上购物平台，这一次题目的目标，就是预测某一个商品在某一天被售出的概率，给定的数据有一段时间内的商品的销售情况（数量，价格，地区，品类，商品的全面俄文描述，商品的图片）等信息。
我的kaggle账号：https://www.kaggle.com/yyqing/competitions
丢了top10% 自己的时间规划出了点问题，工作上临时加了点任务，所以玩比赛这边就少花了点时间，水平也不到位，本来在勉强拿到10%的最后一名，当剔除违规竞赛者之后，排在了10%之外。:(
很少有题目同时用到了GBDT,CV,NLP的，这次算是遇到一个，算是涨了见识，有人取胜重点靠抽取到商品图像的特征，有人取胜重点靠全量商品描述的词向量，有人取胜在于强大的ensemble各路模型各路特征。这一次我入场比较晚，找人组队就很难了，一个人打还是比较累的，虽然利用到了之前储存好的各种代码片段，不过还是时间太紧，精力有限，再加上我是球迷，我德表现如此糟糕，于是，哎哟喂，我也表现如此糟糕
自己的收获 短短10天，虽然成绩不佳，但是知识和技能上的收获还是感觉满满的：
组合模型在这一题发挥了重要的作用，我除了自己训练了xgb，lgb和catboost之外，还从讨论区拷贝来了rnn，ridge regression的成果 自己没怎么实践过自然语言处理，照猫画虎罗列好了tfidf，pca，svd等等，微调一下ngram，对自己分数提高还有有用的 认真按照自己的思路对着各路category实践了target encode，感觉用处不大，过拟合有点重了，后来在讨论中得知，大神们用的高阶的target encode还有加噪音加平滑，据说也有用，下次我也玩高级的TE 由于是第一次搞词向量的题目，第一次实践了稀疏矩阵，lgb和xgb都支持，catboost还不支持，不过用pca将稀疏矩阵做成少量几列还是可以喂给catboost用 拼接各种数据的时候，numpy的运算速度比dataframe要快，有的时候直接用pandas做数学运算，慢到出奇，不如把数据先导出到numpy，运算完再重填回dataframe，节约时间大于一个数量级 压缩数据在内存中占的体积，link这个参考很不错 多线程提取图像特征，参考这里，让我差不多一天时间就把大约180万张图的基本特征都拿出来了。 在处理大量小图片的时候，固态硬盘比机械硬盘的优势就显示出来了，机械硬盘的IOPS实在是跟不上cpu gpu的推理速度，分分钟就到瓶颈，不过GCP上的几百GB ssd价格好贵啊，玩了一会儿做完图像就赶紧换回了机械硬盘。 高分答案集锦 这一次由于数据的丰富性，值得学习的思路很多，且很多大神分享的思路会有部分重合，这里只摘抄我认为最值得学习并且去重的部分思路
1st, Little Boat, Dance with Ensemble 有一些lgb，有一些nn，有一些xgb作为第一层，第二层还是一些lgb+nn+xgb,第三层就是一个nn了。不过这么组合出来分数提升了大约0.0002到0.0004，和直接线性组合比起来，可能差不多。
最佳单模型的nn和lgb都做到了215x的分数，另外队友加来一个大量商品描述+RNN做出来的特征，给lgb分数boost到了213x。
stacking在这一局非常重要，模型的多样化是取胜的关键，
神经网络 我自己是重点时间都放NN了，所以也没怎么做特征工程。我就来分享一下如何用一个nn做到215x的吧。
数字特征加上类别特征，分数有0.227x 加上了title和description都用rnn训练一下，再加上fastText预训练的embedding，调整一下，分数到了0.221x 自训练的fastText embedding，在全量数据上，分数到了0.220x 加上vgg16的top layer的平均池化，这个让分数变差了。tuning一下，在合并文本、图像、类别、数字特征之前，分别加了一层，有了效果，0.219x了 尝试调整文本模型，CNN或者attention等等，没有一个work。最后，两层的LSTM加一个dense，提高了分数0.0003 给图像尝试不同的CNN，没有一个“fine tune”的模型有用，并且还花了很多时间。最后发现fixed ResNet50的中间层有用，提高了0.0005， 到了0.218x了 开始各种tuning，都是基于自己想法的去调，发现在text和LSTM之间加上spatial dropout超级有用，提升了大约0.0007到0.001，又结合调整了dropout的比例，提升了0.001到0.0015，分数到了0.2165到0.217 开始把队友做的特征都加进来，队友做的文字类的特征加进来都没啥用，不过其他类的特征都有用，最后一个NN做到0.215x 如果一路上把所有模型都存下来（少量特征训练的模型，加了特征分数变差的模型），然后再训练一个全连接on top of those,可以得到0.008的提升，这个分数可以进入前10名了 秀一下我的模型架构图吧： 评论区摘抄 我在评论中看到他有说，keras的RNN模型，用CuDNN实现的来运行的话，会快很多。
然后他贴出了他的rnn代码：
seq_title_description = Input(shape=[max_seq_description_length], name=&amp;quot;seq_description&amp;quot;) emb_seq_title_description = Embedding(vocab_size, EMBEDDING_DIM1, weights=[embedding_matrix1], trainable=False)( seq_title_description) emb_seq_title_description = SpatialDropout1D(0.</description></item><item><title>kaggle模型组合指南</title><link>https://yyq.github.io/posts/2018/2018-05-14-kaggle-ensemble-guide/</link><pubDate>Mon, 14 May 2018 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2018/2018-05-14-kaggle-ensemble-guide/</guid><description>前言 本文大部分干货内容，翻译自 kaggle ensembling guide,感谢。
半年前刚刚开始玩kaggle的时候,讨论区，kernel区，获奖分享等等就有很多人发出关键字ensemble，blend之类的。当想要深入了解其中,许多人首推这个博客链接：https://mlwave.com/kaggle-ensembling-guide/, 随意瞥一眼，看上去有用。自己实践的时候，只用了多个模型预测结果的线性组合，有效果。于是计划了要仔细从头到尾把这篇长博客撸一遍，今天终于有空来，翻译一下，方便以后查阅。
模型组合是一个非常强大的黑科技，在很多类型的机器学习任务中都可以用来提升准确率。这篇文章我将会分享我在kaggle比赛中用到的各种组合方法。
文章的第一部分，我会介绍从submission文件作出组合来，第二部分我会介绍用融合的方法。
我会解释为什么组合会降低general的错误。最后我会展示不同的组合方法，给出一些代码，你也可以自己试试.
This is how you win ML competitions: you take other peoples’ work and ensemble them together. by Vitaly Kuznetsov NIPS2014
从submission文件创建组合 最基本的最方便的组合方式，就是从提交的csv文件开始。你只需要那些预测的结果文件，不需要重新训练模型。这是对已经存在的模型最快的组合方式，在组队的时候玩起来更high。
投票组合 我们先来看看简单的投票玩法。了解为什么模型组合可以降低错误率，为什么用低相关性的模型结果的组合效果更好。
纠错码 在某个竞赛任务重，所有的信号都正确的排列，是非常重要的。如果我们有一个二进制信号序列如下： 1110110011101111011111011011 然而我们得到的是有一点点被破坏了的版本（有一位被取反了）： 1010110011101111011111011011
一个机器学习例子 投票者的数量 相关性 在热带雨林覆盖类型预测题目中的应用</description></item><item><title>kaggle | talkingdata-adtracking-fraud-detection top 4%</title><link>https://yyq.github.io/posts/2018/2018-05-09-talking-data/</link><pubDate>Wed, 09 May 2018 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2018/2018-05-09-talking-data/</guid><description>题目链接:kaggle, talkingdata是一家中国公司，我将其理解成一个第三方的移动数据平台，移动端的广告分发是其一个重要业务，其中会出现一些恶意的点击，这个比赛的意义在于：他们想要通过机器学习的方式抓出其中的欺诈类型的点击。
我的kaggle账号：https://www.kaggle.com/yyqing/competitions
纪念一下自己的第一个solo 银牌 之前有个比赛进过前5%的，不过由于用了两个账号在轮番提交，被取消成绩了。这次算是记住教训了，就一个账号玩，依然solo，排名122/3967，public LB score 0.9898130, private LB 0.9821300, mark一下，离自己的mater目标迈出了重要的一步。另外值得庆幸的是，这是第若干次比赛成绩出来后，我的私榜排名是比公榜排名进步的，证明我的模型的泛化能力一直还行，至少是领先于kaggle上大家的平均水平的。
这次比赛的最后一次ensemble结果提交是在回湖南的火车上完成的，正好看到kaggle社区总积分的新晋No.1&amp;ndash;bestfitting的访谈博客，这位同学竟然是长沙人，也是蛮意外的。我努力向老乡学习吧。
自评：又一场内存的战役 学到了很多，这些知识大概不会有哪本书籍有写，也不会有哪个视频教程说，都是自己摸爬滚打得来的，主要是对于大量数据的存，取，和运算。公司也没有给我集群机器玩的情况下，只能自己在谷歌云上玩了。当自费的玩大容量云主机的时候，全都按时收费，每一个GB内存和硬盘都要收费，每一个cpu core都收费的情况下，我会格外的在意，我会最大化的利用cpu，内存等&amp;quot;昂贵&amp;quot;资源。穷嘛，省时间就是省钱。
接下来就说一些在有关方面的技巧吧
习惯性的del不再使用的数据，then gc.collect() could save your asshole. 很多时候调整模型都要考虑使用或者更改不同的特征，当你有几十个特征，每个特征的存放都会占用GB级别的资源的时候，不要尝试一次性处理所有的特征。一个一个来，180Million的数据，一个特征，一次开一个进程，只计算一个特征，比如groupby过程中会爆炸到20多GB的内存，虽然最后计算完结果只有1GB内存。 不要尝试将包含所有特征的大表存成一个文件，将每一个特征保存成一个单独文件。之后要用到这个特征的时候，再读取进来join到dataframe中。提高灵活性，用哪些我就只载入哪些。 pandas读取大体积（1GB或者更大）csv文件速度很慢，特别是当我有几十个特征，每个特征都有一两GB的时候，pandas读取一个30GB的csv文件所花费的时间，绝对会让你崩溃。这个时候pandas.read_feather()和pandas.to_feather()是一个很好的方式，直接把一个dataframe的内容，以feather的格式，从内存dump到磁盘，不用管csv的index等等相关参数，就是dump什么样的内容到磁盘，读取回来还是什么内容在内存，重点是，速度比read_csv快了一个数量级。 关于pandas有点慢的问题，期间还认真尝试过dask和pandas on ray两个开源工具，dask功能齐全一些，在有一些操作比pandas快很多，不过在这次实践中对我的时间节约没有太多的帮助，所以没用。pandas on ray的话，显然还不太成熟，各种常用函数缺失，遂放弃。 每一次探索新特征的时候，一定要有一个较小的可用的可靠的快速的validation的方案，来验证新的特征是否有效。不然每次都用全量的数据来计算，时间成本太高。 每一次运行时间较长的实验（比如全量数据，比如超小的学习率）之前，一定得有一个debug pipeline，快速走完整个过程。 养成随时save model的好习惯。之前做图像深度学习的时候还知道存下很多个epoch完毕之后的模型文件，没成想lightGBM在遇到稍微大点的数据集的也会有各种坑。比如有时候会遇到训练到一半，内存炸了，程序崩了，比如有时候会train完毕后没有及时清理删除训练数据集，predict的时候内存炸了。想起mxnet的李沐博士的视频里说的，我就是不小心把整栋机房弄停电了嘛，你们跑了三四天的程序都不保存中间模型，那能怪我喽，谁叫你们不随时保存的。 哎呀，说多了都是泪，对于我个人来说都是浪费的钱啊，以后想起来什么泪点，我再补充到这个段落来。 高分答案集锦 接下来欣赏欣赏别人的思路吧
3rd, bestfitting, NN based solution 在同时做另外一个google landmark的题目，正好发现最近的NN模型可以用，就在这里也直接用NN了。（大神就是可以模型广泛应用到各个业务上，难道以后深度模型要统治所有，彻底打败树么）。他的主要尝试了的是23个特征，用single LGBM做到了在public LB分数0.9817。（这分数比我最终提交的分数还高），这是他第一次在kaggle里用LGBM。
设计了基于那23个特征，并且适当处理后(NA, out of vocabulary, scale)，用NN做到了公榜0.9820，然后论坛上发现click delta是个重要线索，于是多做了前五个和后五个click的时间序列加到网络里，用RNN来寻找点击序列的模式，模型可以做到公榜9821了。然后设计了不同的神经网络来增加多样性，都是很简单的，比如增加一些残差链接到dense层。四个模型，不单独考虑分数，只考虑综合起来的多样性带来的提升。在组装了神经网络和lgbm模型的成绩之后，分数到了9827。
再加上n-fold和全部数据。稍微有点花时间了，一个fold就两个多小时，一块1080i的GPU.
然后用之前的模型在整个数据集上预测的结果，再加上一些特征（基于IP的，app-os-channel），做了第二层神经网络,分数到了9833。最后的这个提升做好的时候，离比赛时间结束只有30个小时了，来不及其他改进了，时间限制，NN模型的弱点，等等，就酱。他用到的一些特征如下：
channel 1011 os 544 hour 472 app 468 ip_app_os_device_day_click_time_next_1 320 app_channel_os_mean_is_attributed 189 ip_app_mean_is_attributed 124 ip_app_os_device_day_click_time_next_2 120 ip_os_device_count_click_id 113 ip_var_hour 94 ip_day_hour_count_click_id 91 ip_mean_is_attributed 74 ip_count_click_id 73 ip_app_os_device_day_click_time_lag1 67 app_mean_is_attributed 67 ip_nunique_os_device 65 ip_nunique_app 63 ip_nunique_os 51 ip_nunique_app_channel 49 ip_os_device_mean_is_attributed 46 device 41 app_channel_os_count_click_id 37 ip_hour_mean_is_attributed 21 (可以看出来几类特征比较有效，click_time_next，target mean code，count，unique count，还有就是大多特征都是基于ip和其他特征组合来的)</description></item><item><title>kaggle | favorita-grocery-sales-forecasting</title><link>https://yyq.github.io/posts/2018/2018-01-17-favorita-grocery-sales-forecasting/</link><pubDate>Wed, 17 Jan 2018 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2018/2018-01-17-favorita-grocery-sales-forecasting/</guid><description>我的kaggle账号:https://www.kaggle.com/yyqing/competitions
输入数据为某连锁商店的各个店铺各个商品的销量，预测接下来16天的个店铺的各个商品的销量
在Public LB做到了3%，等privateLB出来之后，掉到了6%，发现提交历史记录里，自己的最好的单模型成绩比自己最后提交的组合模型答案成绩还好，可是最好的但模型成绩在publicLB都没有进入10%,心里慌啊，用尽全力的调整到3%啊。终于，自己对LB做到了overfit了，呵呵，教训啊！我的最好单模型成绩public LB 510, private LB 518, 组合模型成绩public LB 508 private LB 519.
1 Lessons learned of myself: 特征工程比模型调参更重要，重要性大出一个数量级 模型调参时，学习率先从较大的数字开始，节约时间 有关日期的比赛，本地cv日期的选择很重要，和最终测试日期有相似性才比较好 本地的每一次运行，cv，参数，分数都需要很好的记录下来，供后期对比分析 很多大神都开始在回归类题目里面开始用NN了，确实成绩会比xgb lgb等会有较大提升 2 Lessons learned from others: 总结两个观点先：
特征工程特别重要 神经网络要胜过决策树boost了 每一段摘要都标记原作者，原文章标题，和原文链接
2.1 Eureka 1st place solution 借鉴了四个别人的模型，cnn，lstm，lgbm，lgbm
只用了2017年的数据，训练集，0531-0719 or 0614-0719, 不同的模型用不同的训练集，验证集，0726-0810
数据处理：把空值和负值都填0
特征工程：
基本特征，分类（store,item,family,class,cluseter）,打折否，day_of_week(only for model 3); 统计特征，时间窗口，最近日期:[1,3,5,7,14,30,60,140]；等时间窗口[1] * 16, [7] * 20; 关键特征：store x item， item， store x class, target： promotion, unit_sales, zeros, 方法：mean,median,max,min,std, day since last appearance.</description></item><item><title>第一次kaggle参赛，铜牌一枚</title><link>https://yyq.github.io/posts/2017/2017-11-29-kaggle/</link><pubDate>Wed, 29 Nov 2017 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2017/2017-11-29-kaggle/</guid><description>https://www.kaggle.com/yyqing/competitions
巴西的汽车保险公司出的题，根据汽车司机的各种信息，来判断明年出险的概率
我的排名Top 7%, 名次 335/5169
在讨论区向诸位大神学习了种种，受益匪浅
实践中，有两类知识，书本和教材中没有，但是非常有效：
奇淫巧技，例如：特征工程时用到的奇奇怪怪的常量；读入内存较大的数据的处理方案；有的时候OHE效果非常明显； 有的观点和思路只存在于有关最新的普通论文中，没有得大奖的也没有入选什么顶级会议的；或者github上某些几乎没有star的工程，拿来实践试试都会有奇效 p.s.
根据某种特定参数下的CV集得出来的答案很有可能对于题目答案过拟合了，这一次考试中你虽然得了高分，然而对实际生活中应用却不会那么具有普适性。他们就像准备考试不是那么充分却考前蒙对了题目而拿了好名次 之前听人说xgboost秒杀很多kaggle题目，可如今就算你拿着最优化的决策树上场，还是会被强大硬件训练出来的神经网络秒的渣都不剩。比如我是xgboost和lgbm组合上场，单次训练个半个小时出来的结果，就是300多名，而第一名的方案就不一样了，5个模型ensemble，一个xgboost，4个神经网络，他的每一个神经网络的训练时间都是好几个小时还都用到了独立显卡，遇到这种强大的对手只能俯首称臣。</description></item><item><title>xgboost模型调试攻略</title><link>https://yyq.github.io/posts/2017/2017-10-23-xgboost-tune/</link><pubDate>Mon, 23 Oct 2017 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2017/2017-10-23-xgboost-tune/</guid><description>最近做题的感受，结合网上看来的资料，总结如下：
xgboost python api 描述 xgboost.DMatrix, 是个类，存的是数据矩阵，自带内存优化，训练加速，可以从numpy.arrays初始化 xgboost.Booster，是个类，模型所在地，包括了较低层次的routines for training,prediction and evalution xgboost.train，是个函数，给定参数来训练booster，返回一个训练完成的booster xgboost.cv，是个函数，给定参数做交叉验证，返回评估历史，list(string) scikit-learn包装的api, xgboost.XGBRegressor, xgboost.XGBClassifier 画图有关的api, xgboost.plot_importance, 根据fitted trees画出重要性，返回的是matplotlib Axes xgboost.plot_tree，画出指定的树 xgboost.to_graphviz,将指定的树转化成graphviz实例 about parameter tune 选用一个相对高一点的learning rate例如0.1，一般选用0.05到0.3之间对于大部分问题都可以了。定了学习率，然后决定合适数量的树，用cv 调整树的特定参数，比如max_depth,min_child_weight,gamma,subsample,colsample_bytree 调整正则化参数例如lambda和alpha，可以降低模型复杂度和提高性能 降低learning rate，再继续优化参数们 tune step by step 第一步，确定learning rate和estimator的数量 不过我们需要初始化一些基本变量，例如：
max_depth=5，一般用3到10，456都是不错的starting points min_child_weight=1，选择较小的值是因为数据分类很不对称并且叶子节点有可能具有较小的group size gamma=0，选择0.1或者0.2都可以，稍后一定需要调试 subsample colsample_bytree=0.8, 典型的选择在0.5到0.9之间 scale_pos_weight=1, 数据类别高度不平衡 learning rate就先用0.1，先用cv来寻找最优的estimators
第二步，max_depth和 min_child_weight 调整着两个是因为它们对模型结果有很大的影响，开始的测试用的范围广泛一点，以后再用较小的范围来确定具体选择为多少。for example: max_depth(3,10,2) min_child_weight(1,6,2)</description></item><item><title>17年9月份机器学习知识进展</title><link>https://yyq.github.io/posts/2017/2017-10-02-machine-learning-ex-progress/</link><pubDate>Mon, 02 Oct 2017 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2017/2017-10-02-machine-learning-ex-progress/</guid><description>刷课1 coursera：
machine learning（matlab手动搭建）基本模型，基本参数，基本调优，98分
deep learning 神经网络，实践调优，来自吴恩达工程实践经验，值得了49美元一个月的票价。前三门课满分拿下，后两门课程还没有出来。
Andrew老师讲的很好，配套练习也很到位，基本知识有了普及。从大概念来说：机器学习是大于深度学习的，而深度学习中，目前应用较广的是神经网络（在有些教程里看到成为多层感知 maybe Multi-Layer-Perceptron?)，然后较为复杂的神经网络模型有卷积神经网络，卷积神经网络中，有各路大神在各路比赛中获得优异成绩的经典模型。
刷课2 Stanford cs231n:李飞飞 CNN-4-image processing 卷积神经网络用于图像识别
她组织收集了ImageNet大量图片的库，并组织了ImageNet比赛，最近几年比赛的冠军成绩识别能力超过人眼，轰动一时，冠军队伍采用了卷积神经网络，因此将深度学习发扬光大，所以很多地方都在讨论”深度学习”，并且将深度学习应用到其他很多场景不仅仅是图像识别 http://image-net.org/index
实践1 tensorflow：走读实践tutorial，基本了解该计算框架 可以自定义数学模型，矩阵运算 第三方已经实现的流行的模型，可拿来即用
刷课3 udacity平台, 乔治亚理工学院的课：machine learning for trading 感觉有点扯远了，很多时间用于讲如何对冲基金量化交易了， 然后简单模型描述了强化学习用于股票交易，目前对我用处不大
实践2 kaggle: 最著名的大数据擂台了，刚入门，学习实践了titanic case 数据预处理，观察数据，简单分析，特征工程处理
计划进一步学习内容 实践方向：
kaggle赛题， 积累python使用经验： tensorflow(模型)，pandas(数据处理)，matplotlib(画图) 理论知识：
等待coursera吴恩达DeepLearning的新课程 coursera Stanford probabilistic-graphical-models 书籍：统计学方法</description></item><item><title>coursera Andrew Ng 机器学习 编程习题总结</title><link>https://yyq.github.io/posts/2017/2017-08-12-machine-learning-exercise/</link><pubDate>Sat, 12 Aug 2017 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2017/2017-08-12-machine-learning-exercise/</guid><description>学习这些知识不是为了学习而学习，是为了解决问题而学习。所以这些知识所配套的编程练习尤为重要了。这门课程做的作业，可算是让我开了眼界，简单的一些代码和并不是特别大量的数据，就能解决一些简单的问题。受益匪浅！记录下来这些，以供之后解决复杂问题的时候给一个参考思路。
这门课程的代码我是用的matlab，我的代码参考链接点击这里
Linear Regression 解决预测房价的问题，已知面积，房间数 写出线性回归的代价函数和梯度下降函数 画图来看，有个直观感受 不同的learning rate有不同的影响，收敛快慢 Logistic Regression 预测某个学生是否通过了某门课程，已知数据包括两次测验的成绩，画出直线决策边界
预测制造工厂出产的芯片的质量合格与否，根据芯片的两次测试成绩，画出像圆形的决策边界
sigmoid函数
代价函数，梯度下降函数
范化
Multi-class classification and Neural Network 数字图片的识别，输入图片，输出数字 通过1-vs-N的逻辑回归函数来识别 通过神经网络来识别(这里只实现了前向反馈用来预测)，theta矩阵已知，效果显然比逻辑回归好 Neural Networks Learning 仍然是识别数字 实现了神经网络里的后向反馈，代价函数，梯度下降，随机初始化，泛化，梯度检查等步骤 可视化隐藏层，有个直观感受 Regularized Linear Regression and Bias v.s. Variance 不同的偏差，方差，通过画图观察 画学习曲线，发现了高偏差问题，数据量增多也不管用，那么线性改多项式 多项式回归，画图，调整选择lambda Support Vector Machines 识别垃圾邮件，亲测自己收到的各种广告垃圾邮件，果然有效！ Informally, the C parameter is a positive value that controls the penalty for misclassified training examples SVM with 高斯内核 决策边界，无比扭曲的曲线都可以适应好 垃圾邮件case:邮件单词预处理，词库预处理，提取邮件特征，训练，预测，GoodJob!</description></item><item><title>coursera Andrew Ng 机器学习第十一周笔记 应用举例：photo OCR</title><link>https://yyq.github.io/posts/2017/2017-08-11-machine-learning-week11/</link><pubDate>Fri, 11 Aug 2017 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2017/2017-08-11-machine-learning-week11/</guid><description>Application example: photo OCR pipeline text detection character segmentation character classification sliding windows too easy to learn Getting more data: artificial data synthesis Discussion on getting more data
make sure you have a low bias classifier before expending the effort.(Plot learning curves.) keep increasing the nuber of features/number of hidden units in neural network until you have a low bias classifier how much work would it be to get 10x as much data as we currently have?</description></item><item><title>coursera Andrew Ng 机器学习第十周笔记 大数据量的机器学习</title><link>https://yyq.github.io/posts/2017/2017-08-10-machine-learning-week10/</link><pubDate>Thu, 10 Aug 2017 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2017/2017-08-10-machine-learning-week10/</guid><description>stochastic gradident descent mini-batch gradient descent Batch gradient descent: use all m examples in each iteration
Stochastic gradient descent: use 1 examples in each iteration
Mini-batch gradient descent: use b examples in each iteration
checking for convergence every 1000 iterations, plot cost(theta, xi, yi) averaged over the last 1000 examples processed by algorithm.
online learning example shiping service website where user comes, specifies origin and destination, you offer to ship their package for some asking price, and user sometimes choose to use your shipping service(y=1), sometimes not(y=0).</description></item><item><title>coursera Andrew Ng 机器学习第九周笔记 异常检测与推荐系统</title><link>https://yyq.github.io/posts/2017/2017-08-09-machine-learning-week9/</link><pubDate>Wed, 09 Aug 2017 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2017/2017-08-09-machine-learning-week9/</guid><description>Anomaly Detection Density estimation Dataset: x1, x2, x3, &amp;hellip; xm Is X-test anomalous ?
algorithm anomaly detection vs supervised learning choosing what features to use non-gaussian features, maybe log(x),sqrt(x),x^2 to gaussian-like features
multivariate G distributon Don&amp;rsquo;t model p(x1) p(x2) separately.
Modle p(x) all in one go.
Recommender Systems collaborative filtering algorithm finding related movies smallest|| xi - xj ||
users who have not rated any movies mean normalization</description></item><item><title>coursera Andrew Ng 机器学习第八周笔记 降低维度</title><link>https://yyq.github.io/posts/2017/2017-08-08-machine-learning-week8/</link><pubDate>Tue, 08 Aug 2017 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2017/2017-08-08-machine-learning-week8/</guid><description>unsupervised learning k-means altorithm clustering, optimization objective clustering, random initialization should have K &amp;lt; m
Randomly pick K training examples.
clustering, choosing the number of clusters, K Please draw a graph. Elbow method
Dimensionality reduction Motivation I: data compressioni
Motivation II: Visualization
Princiapl Component Analysis reduce from n-dimension to k-dimension: find k vectors u1 u2 u3 uk onto which to project the data,so as to minimize the projection error.
PCA steps</description></item><item><title>coursera Andrew Ng 机器学习第七周笔记 支持向量机</title><link>https://yyq.github.io/posts/2017/2017-08-07-machine-learning-week7/</link><pubDate>Mon, 07 Aug 2017 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2017/2017-08-07-machine-learning-week7/</guid><description>Support Vector Machines optimization Objective Large Margin Intuition Kernels Using an SVM choice of parameter C
choice of kernel(similarity function)
if Gaussian Kernel, need to choose sigma^2
Not all similarity functions make valid kernels(need to satisfy technical condition called &amp;ldquo;Mercer&amp;rsquo;s Theorem&amp;rdquo; to make sure SVM packages' optimizations run correctly, and do not diverge).
Polynomial kernel: More esoteric: String kernel, chi-square kernel, histogram intersection kernel
multi-class calssification
K SVMs, one to distinguish one from the rest.</description></item><item><title>coursera Andrew Ng 机器学习第六周笔记 系统设计与优化技巧</title><link>https://yyq.github.io/posts/2017/2017-08-06-machine-learning-week6/</link><pubDate>Sun, 06 Aug 2017 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2017/2017-08-06-machine-learning-week6/</guid><description>Advice for Applying Machine Learning 重要的笔记，必须手写上传 Machine Learning System Design email spam example Given a data set of emails, we could construct a vector for each email. Each entry in this vector represents a word. The vector normally contains 10,000 to 50,000 entries gathered by finding the most frequently used words in our data set. If a word is to be found in the email, we would assign its respective entry a 1, else if it is not found, that entry would be a 0.</description></item><item><title>coursera Andrew Ng 机器学习第五周笔记 神经网络的学习过程</title><link>https://yyq.github.io/posts/2017/2017-08-05-machine-learning-week5/</link><pubDate>Sat, 05 Aug 2017 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2017/2017-08-05-machine-learning-week5/</guid><description>Neural Networks: Learning cost function L = total number of layers in the network sl = number of units (not counting bias unit) in layer l K = number of output units/classes the double sum simply adds up the logistic regression costs calculated for each cell in the output layer the triple sum simply adds up the squares of all the individual Θs in the entire network. the i in the triple sum does not refer to training example i Backpropagation Algorithm Backpropagation in Practice thetaVector = [ Theta1(:); Theta2(:); Theta3(:); ] deltaVector = [ D1(:); D2(:); D3(:) ] Theta1 = reshape(thetaVector(1:110),10,11) Theta2 = reshape(thetaVector(111:220),10,11) Theta3 = reshape(thetaVector(221:231),1,11) gradient checking Once you have verified once that your backpropagation algorithm is correct, you don&amp;rsquo;t need to compute gradApprox again.</description></item><item><title>coursera Andrew Ng 机器学习第四周笔记 神经网络模型</title><link>https://yyq.github.io/posts/2017/2017-08-04-machine-learning-week4/</link><pubDate>Fri, 04 Aug 2017 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2017/2017-08-04-machine-learning-week4/</guid><description>Neural Networks:Representation 这个周的课程很简单，就是介绍了神经网络是什么样子的
introduction examples multi classification example</description></item><item><title>coursera Andrew Ng 机器学习第三周笔记 逻辑回归与泛化</title><link>https://yyq.github.io/posts/2017/2017-08-03-machine-learning-week3/</link><pubDate>Thu, 03 Aug 2017 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2017/2017-08-03-machine-learning-week3/</guid><description>Logistic Regression Classification The classification problem is just like the regression problem, except that the values y we now want to predict take on only a small number of discrete values. For now, we will focus on the binary classification problem in which y can take on only two values, 0 and 1.
Logistic Regression Model Decision Boundary The decision boundary is the line that separates the area where y = 0 and where y = 1.</description></item><item><title>coursera Andrew Ng 机器学习第二周笔记 多个变量的线性回归</title><link>https://yyq.github.io/posts/2017/2017-08-02-machine-learning-week2/</link><pubDate>Wed, 02 Aug 2017 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2017/2017-08-02-machine-learning-week2/</guid><description>Linear Regression with Multiple Variables Multiple Features Gradient Descent for Multiple Variables Feature Scaling We can speed up gradient descent by having each of our input values in roughly the same range.
feature scaling: involves dividing the input values by the range of the input variable resulting in a new range of just 1. mean normalization: involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero.</description></item><item><title>coursera Andrew Ng 机器学习第一周笔记 线性回归</title><link>https://yyq.github.io/posts/2017/2017-08-01-machine-learning-week1/</link><pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2017/2017-08-01-machine-learning-week1/</guid><description>machine learning supervised learning, unsupervised learning, others(reinforcement learning, recommender system)
supervised learning regressing，predict input into continuous result classification, predict input into distract result examples：
given size, rooms, predict house price, like 1M, 100Grand, 1B house prize is expensive or cheap ? given picture with human, predict his age: 0.1，12.5，78.8 given a patient with tumor, predict it&amp;rsquo;s a malignant or benign unsupervised learning problems with little or no idea what our results should look like</description></item></channel></rss>