<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>coursera on Young Story</title><link>https://yyq.github.io/tags/coursera/</link><description>Recent content in coursera on Young Story</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 12 Aug 2017 00:00:00 +0000</lastBuildDate><atom:link href="https://yyq.github.io/tags/coursera/index.xml" rel="self" type="application/rss+xml"/><item><title>coursera Andrew Ng 机器学习 编程习题总结</title><link>https://yyq.github.io/posts/2017/2017-08-12-machine-learning-exercise/</link><pubDate>Sat, 12 Aug 2017 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2017/2017-08-12-machine-learning-exercise/</guid><description>学习这些知识不是为了学习而学习，是为了解决问题而学习。所以这些知识所配套的编程练习尤为重要了。这门课程做的作业，可算是让我开了眼界，简单的一些代码和并不是特别大量的数据，就能解决一些简单的问题。受益匪浅！记录下来这些，以供之后解决复杂问题的时候给一个参考思路。
这门课程的代码我是用的matlab，我的代码参考链接点击这里
Linear Regression 解决预测房价的问题，已知面积，房间数 写出线性回归的代价函数和梯度下降函数 画图来看，有个直观感受 不同的learning rate有不同的影响，收敛快慢 Logistic Regression 预测某个学生是否通过了某门课程，已知数据包括两次测验的成绩，画出直线决策边界
预测制造工厂出产的芯片的质量合格与否，根据芯片的两次测试成绩，画出像圆形的决策边界
sigmoid函数
代价函数，梯度下降函数
范化
Multi-class classification and Neural Network 数字图片的识别，输入图片，输出数字 通过1-vs-N的逻辑回归函数来识别 通过神经网络来识别(这里只实现了前向反馈用来预测)，theta矩阵已知，效果显然比逻辑回归好 Neural Networks Learning 仍然是识别数字 实现了神经网络里的后向反馈，代价函数，梯度下降，随机初始化，泛化，梯度检查等步骤 可视化隐藏层，有个直观感受 Regularized Linear Regression and Bias v.s. Variance 不同的偏差，方差，通过画图观察 画学习曲线，发现了高偏差问题，数据量增多也不管用，那么线性改多项式 多项式回归，画图，调整选择lambda Support Vector Machines 识别垃圾邮件，亲测自己收到的各种广告垃圾邮件，果然有效！ Informally, the C parameter is a positive value that controls the penalty for misclassified training examples SVM with 高斯内核 决策边界，无比扭曲的曲线都可以适应好 垃圾邮件case:邮件单词预处理，词库预处理，提取邮件特征，训练，预测，GoodJob!</description></item><item><title>coursera Andrew Ng 机器学习第十一周笔记 应用举例：photo OCR</title><link>https://yyq.github.io/posts/2017/2017-08-11-machine-learning-week11/</link><pubDate>Fri, 11 Aug 2017 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2017/2017-08-11-machine-learning-week11/</guid><description>Application example: photo OCR pipeline text detection character segmentation character classification sliding windows too easy to learn Getting more data: artificial data synthesis Discussion on getting more data
make sure you have a low bias classifier before expending the effort.(Plot learning curves.) keep increasing the nuber of features/number of hidden units in neural network until you have a low bias classifier how much work would it be to get 10x as much data as we currently have?</description></item><item><title>coursera Andrew Ng 机器学习第十周笔记 大数据量的机器学习</title><link>https://yyq.github.io/posts/2017/2017-08-10-machine-learning-week10/</link><pubDate>Thu, 10 Aug 2017 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2017/2017-08-10-machine-learning-week10/</guid><description>stochastic gradident descent mini-batch gradient descent Batch gradient descent: use all m examples in each iteration
Stochastic gradient descent: use 1 examples in each iteration
Mini-batch gradient descent: use b examples in each iteration
checking for convergence every 1000 iterations, plot cost(theta, xi, yi) averaged over the last 1000 examples processed by algorithm.
online learning example shiping service website where user comes, specifies origin and destination, you offer to ship their package for some asking price, and user sometimes choose to use your shipping service(y=1), sometimes not(y=0).</description></item><item><title>coursera Andrew Ng 机器学习第九周笔记 异常检测与推荐系统</title><link>https://yyq.github.io/posts/2017/2017-08-09-machine-learning-week9/</link><pubDate>Wed, 09 Aug 2017 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2017/2017-08-09-machine-learning-week9/</guid><description>Anomaly Detection Density estimation Dataset: x1, x2, x3, &amp;hellip; xm Is X-test anomalous ?
algorithm anomaly detection vs supervised learning choosing what features to use non-gaussian features, maybe log(x),sqrt(x),x^2 to gaussian-like features
multivariate G distributon Don&amp;rsquo;t model p(x1) p(x2) separately.
Modle p(x) all in one go.
Recommender Systems collaborative filtering algorithm finding related movies smallest|| xi - xj ||
users who have not rated any movies mean normalization</description></item><item><title>coursera Andrew Ng 机器学习第八周笔记 降低维度</title><link>https://yyq.github.io/posts/2017/2017-08-08-machine-learning-week8/</link><pubDate>Tue, 08 Aug 2017 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2017/2017-08-08-machine-learning-week8/</guid><description>unsupervised learning k-means altorithm clustering, optimization objective clustering, random initialization should have K &amp;lt; m
Randomly pick K training examples.
clustering, choosing the number of clusters, K Please draw a graph. Elbow method
Dimensionality reduction Motivation I: data compressioni
Motivation II: Visualization
Princiapl Component Analysis reduce from n-dimension to k-dimension: find k vectors u1 u2 u3 uk onto which to project the data,so as to minimize the projection error.
PCA steps</description></item><item><title>coursera Andrew Ng 机器学习第七周笔记 支持向量机</title><link>https://yyq.github.io/posts/2017/2017-08-07-machine-learning-week7/</link><pubDate>Mon, 07 Aug 2017 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2017/2017-08-07-machine-learning-week7/</guid><description>Support Vector Machines optimization Objective Large Margin Intuition Kernels Using an SVM choice of parameter C
choice of kernel(similarity function)
if Gaussian Kernel, need to choose sigma^2
Not all similarity functions make valid kernels(need to satisfy technical condition called &amp;ldquo;Mercer&amp;rsquo;s Theorem&amp;rdquo; to make sure SVM packages' optimizations run correctly, and do not diverge).
Polynomial kernel: More esoteric: String kernel, chi-square kernel, histogram intersection kernel
multi-class calssification
K SVMs, one to distinguish one from the rest.</description></item><item><title>coursera Andrew Ng 机器学习第六周笔记 系统设计与优化技巧</title><link>https://yyq.github.io/posts/2017/2017-08-06-machine-learning-week6/</link><pubDate>Sun, 06 Aug 2017 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2017/2017-08-06-machine-learning-week6/</guid><description>Advice for Applying Machine Learning 重要的笔记，必须手写上传 Machine Learning System Design email spam example Given a data set of emails, we could construct a vector for each email. Each entry in this vector represents a word. The vector normally contains 10,000 to 50,000 entries gathered by finding the most frequently used words in our data set. If a word is to be found in the email, we would assign its respective entry a 1, else if it is not found, that entry would be a 0.</description></item></channel></rss>