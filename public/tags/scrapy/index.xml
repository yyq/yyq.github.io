<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>scrapy on Young Story</title><link>https://yyq.github.io/tags/scrapy/</link><description>Recent content in scrapy on Young Story</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 16 Feb 2017 00:00:00 +0000</lastBuildDate><atom:link href="https://yyq.github.io/tags/scrapy/index.xml" rel="self" type="application/rss+xml"/><item><title>python scrapy爬虫练手</title><link>https://yyq.github.io/posts/2017/2017-02-16-python-crawler-practice/</link><pubDate>Thu, 16 Feb 2017 00:00:00 +0000</pubDate><guid>https://yyq.github.io/posts/2017/2017-02-16-python-crawler-practice/</guid><description>简介 初步了解和实践了一下用python写个爬虫，有现成的库，学起来的方便。
用的爬虫框架是：scrapy 官网链接
参考的网页是：Segmentfault.com的这篇文章
我的代码存放在：GitHub link
新增加的技能点 基本了解scrapy的用法，爬虫的最基本的思路 python 3的语法里，print是一定要有括号的 xpath基础知识掌握和应用，简单的抓取用xpath基本够了，不过以后要来高精专的字符提取，还是得精通正则 python yield关键字的了解，它通常会出现在某个generator函数里，当这个generator函数执行的时候，遇到yield表达式，就会执行这个表达式，并且将其结果当做返回值return yield在某种情况下非常有用：有很多很多的条目，只需要读取一次。可以避免生成超大的数组。当你有很多大量的元素需要访问一次的时候，全部（或者部分）直接存到内存里都不是高效的做法，高效的做法是一次只载入内存一个，用完了之后就等待，当需要访问下一个元素的时候，才启动这个函数，去处理下一个元素。 scrapy工程需要注意一下，project名称和爬虫名称别用同一个单词，不然有些文件里需要引用其他文件夹的class的时候，会出错</description></item></channel></rss>