<!doctype html><html lang=en-us>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge,chrome=1">
<title>deeplearning.ai之卷积神经网络课程总结 | Young Story</title>
<meta name=viewport content="width=device-width,minimum-scale=1">
<meta name=description content="等了一个月，coursera上的deeplearning.ai的第四门课程终于放出来了，月初忙了几天没空，这两天有空，认认真真刷掉了这门课（看过每一个视频，理解到位每一段视频内容，选择题作业满分，编程作业以及附加作业全部pass）。
在知乎上看过了https://zhuanlan.zhihu.com/p/21930884，中文版的CS231n(by stanford Li FeiFei)的全部notes，受益匪浅，再补一波吴恩达老师的此课，颇有收获。
知识点总结如下：
第一周 卷积神经网络 Understand the convolution operation Understand the pooling operation Remember the vocabulary used in convolutional neural network (padding, stride, filter, ...) Build a convolutional neural network for image multi-class classification  简介计算机视觉 通过简单的矩阵来卷积，进行边缘检测，本科时的数字图像处理课程学过，这个简单 padding, 为了卷积时不把边角的像素忽略掉，得按照卷积核的大小在图像周围一圈补充像素，常用的有两种， valid padding也就是no padding，input: n*n with filter f*f, output: (b-f+1)*(n-f+1) same padding,也就是输出矩阵和输入矩阵一样大，这样来计算的话n+2p-f+1=n,所以p=(f-1)/2,啊哈，所以啊，最好一般你的filter都用奇数，搞了偶数那same padding就不好做了哈 stride,步长，按照字面意思就好理解了，为了卷积的同时做采样  input: n*n with filder f*f padding=p ouput is : round((n+2p-f)/s+1), round为向下取整   三维矩阵的卷积，当时彩色图像的时候，比如一个图像是6*6*3，6乘以6的矩阵是图像大小，3是channel的数量，那么filter不能再是3乘以3了，得用3*3*3了，前面两个3是filer的高度和宽度，第三维的3也是channel数，必须和图像的channel一样匹配好才是.">
<meta name=generator content="Hugo 0.91.2">
<meta name=ROBOTS content="NOINDEX, NOFOLLOW">
<link rel=stylesheet href=/ananke/css/main.min.css>
<meta property="og:title" content="deeplearning.ai之卷积神经网络课程总结">
<meta property="og:description" content="等了一个月，coursera上的deeplearning.ai的第四门课程终于放出来了，月初忙了几天没空，这两天有空，认认真真刷掉了这门课（看过每一个视频，理解到位每一段视频内容，选择题作业满分，编程作业以及附加作业全部pass）。
在知乎上看过了https://zhuanlan.zhihu.com/p/21930884，中文版的CS231n(by stanford Li FeiFei)的全部notes，受益匪浅，再补一波吴恩达老师的此课，颇有收获。
知识点总结如下：
第一周 卷积神经网络 Understand the convolution operation Understand the pooling operation Remember the vocabulary used in convolutional neural network (padding, stride, filter, ...) Build a convolutional neural network for image multi-class classification  简介计算机视觉 通过简单的矩阵来卷积，进行边缘检测，本科时的数字图像处理课程学过，这个简单 padding, 为了卷积时不把边角的像素忽略掉，得按照卷积核的大小在图像周围一圈补充像素，常用的有两种， valid padding也就是no padding，input: n*n with filter f*f, output: (b-f+1)*(n-f+1) same padding,也就是输出矩阵和输入矩阵一样大，这样来计算的话n+2p-f+1=n,所以p=(f-1)/2,啊哈，所以啊，最好一般你的filter都用奇数，搞了偶数那same padding就不好做了哈 stride,步长，按照字面意思就好理解了，为了卷积的同时做采样  input: n*n with filder f*f padding=p ouput is : round((n+2p-f)/s+1), round为向下取整   三维矩阵的卷积，当时彩色图像的时候，比如一个图像是6*6*3，6乘以6的矩阵是图像大小，3是channel的数量，那么filter不能再是3乘以3了，得用3*3*3了，前面两个3是filer的高度和宽度，第三维的3也是channel数，必须和图像的channel一样匹配好才是.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://yyq.github.io/posts/2017/2017-11-13-cnn/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2017-11-13T00:00:00+00:00">
<meta property="article:modified_time" content="2017-11-13T00:00:00+00:00">
<meta itemprop=name content="deeplearning.ai之卷积神经网络课程总结">
<meta itemprop=description content="等了一个月，coursera上的deeplearning.ai的第四门课程终于放出来了，月初忙了几天没空，这两天有空，认认真真刷掉了这门课（看过每一个视频，理解到位每一段视频内容，选择题作业满分，编程作业以及附加作业全部pass）。
在知乎上看过了https://zhuanlan.zhihu.com/p/21930884，中文版的CS231n(by stanford Li FeiFei)的全部notes，受益匪浅，再补一波吴恩达老师的此课，颇有收获。
知识点总结如下：
第一周 卷积神经网络 Understand the convolution operation Understand the pooling operation Remember the vocabulary used in convolutional neural network (padding, stride, filter, ...) Build a convolutional neural network for image multi-class classification  简介计算机视觉 通过简单的矩阵来卷积，进行边缘检测，本科时的数字图像处理课程学过，这个简单 padding, 为了卷积时不把边角的像素忽略掉，得按照卷积核的大小在图像周围一圈补充像素，常用的有两种， valid padding也就是no padding，input: n*n with filter f*f, output: (b-f+1)*(n-f+1) same padding,也就是输出矩阵和输入矩阵一样大，这样来计算的话n+2p-f+1=n,所以p=(f-1)/2,啊哈，所以啊，最好一般你的filter都用奇数，搞了偶数那same padding就不好做了哈 stride,步长，按照字面意思就好理解了，为了卷积的同时做采样  input: n*n with filder f*f padding=p ouput is : round((n+2p-f)/s+1), round为向下取整   三维矩阵的卷积，当时彩色图像的时候，比如一个图像是6*6*3，6乘以6的矩阵是图像大小，3是channel的数量，那么filter不能再是3乘以3了，得用3*3*3了，前面两个3是filer的高度和宽度，第三维的3也是channel数，必须和图像的channel一样匹配好才是."><meta itemprop=datePublished content="2017-11-13T00:00:00+00:00">
<meta itemprop=dateModified content="2017-11-13T00:00:00+00:00">
<meta itemprop=wordCount content="506">
<meta itemprop=keywords content="deep learning,cnn,image,"><meta name=twitter:card content="summary">
<meta name=twitter:title content="deeplearning.ai之卷积神经网络课程总结">
<meta name=twitter:description content="等了一个月，coursera上的deeplearning.ai的第四门课程终于放出来了，月初忙了几天没空，这两天有空，认认真真刷掉了这门课（看过每一个视频，理解到位每一段视频内容，选择题作业满分，编程作业以及附加作业全部pass）。
在知乎上看过了https://zhuanlan.zhihu.com/p/21930884，中文版的CS231n(by stanford Li FeiFei)的全部notes，受益匪浅，再补一波吴恩达老师的此课，颇有收获。
知识点总结如下：
第一周 卷积神经网络 Understand the convolution operation Understand the pooling operation Remember the vocabulary used in convolutional neural network (padding, stride, filter, ...) Build a convolutional neural network for image multi-class classification  简介计算机视觉 通过简单的矩阵来卷积，进行边缘检测，本科时的数字图像处理课程学过，这个简单 padding, 为了卷积时不把边角的像素忽略掉，得按照卷积核的大小在图像周围一圈补充像素，常用的有两种， valid padding也就是no padding，input: n*n with filter f*f, output: (b-f+1)*(n-f+1) same padding,也就是输出矩阵和输入矩阵一样大，这样来计算的话n+2p-f+1=n,所以p=(f-1)/2,啊哈，所以啊，最好一般你的filter都用奇数，搞了偶数那same padding就不好做了哈 stride,步长，按照字面意思就好理解了，为了卷积的同时做采样  input: n*n with filder f*f padding=p ouput is : round((n+2p-f)/s+1), round为向下取整   三维矩阵的卷积，当时彩色图像的时候，比如一个图像是6*6*3，6乘以6的矩阵是图像大小，3是channel的数量，那么filter不能再是3乘以3了，得用3*3*3了，前面两个3是filer的高度和宽度，第三维的3也是channel数，必须和图像的channel一样匹配好才是.">
</head>
<body class="ma0 avenir bg-near-white">
<header>
<div class=bg-black>
<nav class="pv3 ph3 ph4-ns" role=navigation>
<div class="flex-l justify-between items-center center">
<a href=/ class="f3 fw2 hover-white no-underline white-90 dib">
Young Story
</a>
<div class="flex-l items-center">
<div class=ananke-socials>
</div>
</div>
</div>
</nav>
</div>
</header>
<main class=pb7 role=main>
<article class="flex-l flex-wrap justify-between mw8 center ph3">
<header class="mt4 w-100">
<aside class="instapaper_ignoref b helvetica tracked">
POSTS
</aside>
<div id=sharing class="mt3 ananke-socials">
</div>
<h1 class="f1 athelas mt3 mb1">deeplearning.ai之卷积神经网络课程总结</h1>
<p class=tracked>
By <strong>
Young
</strong>
</p>
<time class="f6 mv4 dib tracked" datetime=2017-11-13T00:00:00Z>November 13, 2017</time>
</header>
<div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><p>等了一个月，coursera上的deeplearning.ai的第四门课程终于放出来了，月初忙了几天没空，这两天有空，认认真真刷掉了这门课（看过每一个视频，理解到位每一段视频内容，选择题作业满分，编程作业以及附加作业全部pass）。</p>
<p>在知乎上看过了<a href=https://zhuanlan.zhihu.com/p/21930884>https://zhuanlan.zhihu.com/p/21930884</a>，中文版的CS231n(by stanford Li FeiFei)的全部notes，受益匪浅，再补一波吴恩达老师的此课，颇有收获。</p>
<p>知识点总结如下：</p>
<h3 id=第一周-卷积神经网络>第一周 卷积神经网络</h3>
<pre tabindex=0><code>Understand the convolution operation
Understand the pooling operation
Remember the vocabulary used in convolutional neural network (padding, stride, filter, ...)
Build a convolutional neural network for image multi-class classification
</code></pre><ul>
<li>简介计算机视觉</li>
<li>通过简单的矩阵来卷积，进行边缘检测，本科时的数字图像处理课程学过，这个简单</li>
<li>padding, 为了卷积时不把边角的像素忽略掉，得按照卷积核的大小在图像周围一圈补充像素，常用的有两种，</li>
<li>valid padding也就是no padding，<code>input: n*n with filter f*f, output: (b-f+1)*(n-f+1)</code></li>
<li>same padding,也就是输出矩阵和输入矩阵一样大，这样来计算的话<code>n+2p-f+1=n</code>,所以<code>p=(f-1)/2</code>,啊哈，所以啊，最好一般你的filter都用奇数，搞了偶数那same padding就不好做了哈</li>
<li>stride,步长，按照字面意思就好理解了，为了卷积的同时做采样</li>
</ul>
<pre tabindex=0><code>input: n*n with filder f*f padding=p
ouput is : round((n+2p-f)/s+1), round为向下取整
</code></pre><ul>
<li>
<p>三维矩阵的卷积，当时彩色图像的时候，比如一个图像是<code>6*6*3</code>，6乘以6的矩阵是图像大小，3是channel的数量，那么filter不能再是3乘以3了，得用<code>3*3*3</code>了，前面两个3是filer的高度和宽度，第三维的3也是channel数，必须和图像的channel一样匹配好才是.</p>
<pre tabindex=0><code>input: image 6*6*3,   filter 3*3*3
output: 4*4，   every 3*3*3 filter on 1*1*3(of image), =&gt; 1 pixel in 4*4

if we have multiple filters, then the output is 4*4*M
</code></pre><p>``</p>
</li>
<li>
<p>每一个过滤器，对应一个偏差b</p>
<pre tabindex=0><code>Z = W a + b     # a:6*6*3,  W:3*3*3*2,  b:1*2, Z:4*4*2
a = g(Z)
</code></pre><pre tabindex=0><code>
](./2017-11-13/cnn-notation.png)

</code></pre></li>
<li>
<p>简单卷积网络举例</p>
</li>
<li>
<p>卷积层</p>
</li>
<li>
<p>池化层</p>
</li>
<li>
<p>全连接层</p>
</li>
<li>
<p>pooling layers, 纯粹为了压缩体积</p>
</li>
<li>
<p><strong>why convolutions, 这个重要了，这是思想，为什么要用卷积网络？</strong></p>
</li>
<li>
<p><strong>parameter sharing, a feature detector(such as a vertical edge detector)that&rsquo;s useful in one part of the image is probably useful in another part of the image</strong></p>
</li>
<li>
<p><strong>Sparsity of connections: In each layer, each output value depends only on a small number of inputs.</strong></p>
</li>
</ul>
<h3 id=第一周-编程作业>第一周 编程作业</h3>
<p>only with numpy, functions are:</p>
<ul>
<li>zero_pad</li>
<li>conv_single_step, one filter with one slice of input volume</li>
<li>conv_forward, all filters with whole input volume, also padding and stride</li>
<li>pool_forward</li>
<li>conv_backward</li>
<li>create_mast_from_window( this is backward for max pooling)</li>
<li>distribute_value(this is backward for average pooling)</li>
</ul>
<p>with TF, code an application to identify SIGNS</p>
<ul>
<li>placeholders: in TF, this is for input data/volumes/matrix)</li>
<li>init_parameters: shape of W1 W2 Wn, in TF, this is trainable variables</li>
<li>forward_propagation: conv-forward, pool-forward, FC-flatten</li>
<li>cost, we use softmax_cross_entropy_with_logits in this case</li>
<li>whole MODEL, combine those before, sess.run</li>
</ul>
<h3 id=第二周-深度卷积模型案例学习>第二周 深度卷积模型，案例学习</h3>
<pre tabindex=0><code>Understand multiple foundational papers of convolutional neural networks
Analyze the dimensionality reduction of a volume in a very deep network
Understand and Implement a Residual network
Build a deep neural network using Keras
Implement a skip-connection in your network
Clone a repository from github and use transfer learning
</code></pre><ul>
<li>
<p>介绍经典网络了，LeNet-5, AlexNet, VGG</p>
</li>
<li>
<p><strong>ResNets</strong>, 大名鼎鼎的残差网络，在激活函数之前直接加上之前的a值，不废话，一图胜千言
<img src=./2017-11-13/residualblock.png alt> <img src=./2017-11-13/residualnetwork.png alt></p>
</li>
<li>
<p><strong>why ResNets work</strong></p>
</li>
<li>
<p>identity function is easy for Residual block to learn
<img src=./2017-11-13/resnet-0.png alt> <img src=./2017-11-13/resnet-1.png alt></p>
</li>
<li>
<p><strong>1x1 convolution</strong>, we can use this to shrink the channels</p>
</li>
<li>
<p><strong>Inception Network</strong>, 这个也很屌了：If I don&rsquo;t konw which kind of filter to use, than let&rsquo;s use all, and let the network choose the right, 直接stack，那么就不可避免的出现了很多channel吧，那么1x1就可以用上了，
<img src=./2017-11-13/inception-0.png alt> <img src=./2017-11-13/inception-1.png alt></p>
</li>
</ul>
<h3 id=第二周-编程作业残差网络>第二周 编程作业，残差网络</h3>
<p>residual netwokrs, transfer learning</p>
<p>with numpy, keras, tensorflow</p>
<ul>
<li>实现identity block</li>
<li>实现convolutional block</li>
<li>建立ResNet-50模型</li>
<li>简单训练一个epoch花了5分钟，太费资源了，作业里意思意思训练一下就好</li>
</ul>
<h3 id=第三周-目标检测>第三周 目标检测</h3>
<pre tabindex=0><code>Understand the challenges of Object Localization, Object Detection and Landmark Finding
Understand and implement non-max suppression
Understand and implement intersection over union
Understand how we label a dataset for an object detection application
Remember the vocabulary of object detection (landmark, anchor, bounding box, grid, ...)
</code></pre><ul>
<li>目标定位，localization是指在一张有一辆汽车的图上，把汽车的位置框出来，detection是指首先要发现图上有多少辆车，然后一一框出来，再加上分类，就是在一个图上找多类物体而不仅仅找汽车了</li>
<li>地标检测（landmark detection）,标记某些特殊点的坐标，比如汽车方框的中心，左上角，比如人的眼睛，人的鼻子，比如人体躯干</li>
<li>滑动窗口检测，呵呵，这个方法直观上容易理解，各种窗口大小，满图像开始找，就是计算量大了点</li>
<li>卷积版滑动窗口检测，和卷积神经网络很像，只是最后的FC改成softmax，输出分类预测的概率好了，原图的普通窗口在卷积网络计算之后，只剩一个像素了，整张图一起卷积，计算量就比上一种方法大大下降了</li>
<li>方框预测，YOLO算法：先把图片按照3x3或者19x19均分，然后定位车的中心点在哪个矩阵里，然后基于这个矩阵的坐标系（x,y,w,h）横坐标纵坐标都是大于0小于1，长和宽可以大于1</li>
<li>IoU, Intersection over union, 两个矩形的交集面积减去并集面积，用于重合矩形剔除</li>
<li>Non-max suppression, 对于一张图上有多个小矩形都检测到了很多辆车，从概率最大的车框开始，应用IoU.</li>
<li>Anchor Boxes, 先定义好几个anchor box，在此之前，每一个物体分配给物体中心所在的小矩形，之后，每一个物体分配给中心点所在的矩形之后，再陪一个最高IoU的anchor box.</li>
<li>region proposals, 先观察图片上的区域分区，segmentation algorithm, 然后再开始运算</li>
<li>R-CNN,提出区域，一次分类一个区域，输出标签和边界方框</li>
<li>Fast R-CNN, 先分区，然后卷积方案，一起来分类</li>
<li>Faster R-CNN，用卷积方案来分区</li>
</ul>
<h3 id=第三周-编程练习-自动驾驶之汽车检测>第三周 编程练习 自动驾驶之汽车检测</h3>
<p>用tensorflow，numpy，pandas，keras</p>
<ul>
<li>yolo_filter_boxes, yolo过滤，应用IoU和non-max suppression和阈值来过滤boxes</li>
<li>实现iou和non_max_suppression</li>
<li>组合一下前面两者</li>
<li>载入之前训练好的模型参数，并且用来预测</li>
</ul>
<h3 id=第四周-1-人脸识别>第四周-1 人脸识别</h3>
<ul>
<li>人脸确认 face verification
<ul>
<li>输入人脸，姓名，ID</li>
<li>输出该人脸是否和该姓名ID匹配</li>
</ul>
</li>
<li>人脸识别 face recognition
<ul>
<li>有一个数据库存了K个人的信息</li>
<li>输入一个人脸</li>
<li>输出一个人是否是这K个人中之一或者一个都不是</li>
</ul>
</li>
<li>one-shot learning，一枪学习？
<ul>
<li>从一个样例中学习，然后再识别输入图像是否是这个人</li>
<li>学习出一个相似函数，给定的两张图，算出相似度来判断是否同一个人</li>
<li><code>d(img1,img2)=degree of difference between images</code></li>
</ul>
</li>
<li>Siamese network，就是用来解决one-shot学习的模型，输入一个人脸，经过几次卷积池化FC之后输出一个一维矩阵f(1),然后第二个图片，输出一个f(2)，两者L2距离比较近的话，就算同一个人了</li>
<li>Triplet Loss
<ul>
<li>假设有三张图片，Anchor, positive, negative APN，训练的过程中，如果随机的选择APN，那么 d(A,P)+alpha&lt;=d(A,N)就会很容易满足，所以我们应该挑选比较“难”一点的图片来进行训练</li>
</ul>
</li>
</ul>
<h3 id=第四周-2-风格转移>第四周-2 风格转移</h3>
<ul>
<li>什么是神经风格转移（neural style transfer）,选择一张图片的内容，选择另外一张图片的风格，然后融合
<img src=./2017-11-13/style-transfer-eg.png alt></li>
<li>损失函数，生成图像和原来的两幅图的loss加起来，当然两种loss计算方式是不一样的，如何计算，这是重点</li>
</ul>
<pre tabindex=0><code>J(G) = alpha * J-content(C, G) + beta * J-style(S, G)
</code></pre><ul>
<li>J-content 内容损失函数，直接按照两张图卷积过后矩阵的欧氏距离来计算了，</li>
</ul>
<ul class=pa0>
<li class=list>
<a href=/tags/deep-learning class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">deep learning</a>
</li>
<li class=list>
<a href=/tags/cnn class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">cnn</a>
</li>
<li class=list>
<a href=/tags/image class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">image</a>
</li>
</ul>
<div class="mt6 instapaper_ignoref">
</div>
</div>
<aside class="w-30-l mt6-l">
</aside>
</article>
</main>
<footer class="bg-black bottom-0 w-100 pa3" role=contentinfo>
<div class="flex justify-between">
<a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://yyq.github.io/>
&copy; Young Story 2022
</a>
<div>
<div class=ananke-socials>
</div></div>
</div>
</footer>
</body>
</html>