<!doctype html><html lang=en-us>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge,chrome=1">
<title>coursera Andrew Ng 机器学习第二周笔记 多个变量的线性回归 | Young Story</title>
<meta name=viewport content="width=device-width,minimum-scale=1">
<meta name=description content="Linear Regression with Multiple Variables Multiple Features Gradient Descent for Multiple Variables Feature Scaling We can speed up gradient descent by having each of our input values in roughly the same range.
 feature scaling: involves dividing the input values by the range of the input variable resulting in a new range of just 1. mean normalization: involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero.">
<meta name=generator content="Hugo 0.91.2">
<meta name=ROBOTS content="NOINDEX, NOFOLLOW">
<link rel=stylesheet href=/ananke/css/main.min.css>
<meta property="og:title" content="coursera Andrew Ng 机器学习第二周笔记 多个变量的线性回归">
<meta property="og:description" content="Linear Regression with Multiple Variables Multiple Features Gradient Descent for Multiple Variables Feature Scaling We can speed up gradient descent by having each of our input values in roughly the same range.
 feature scaling: involves dividing the input values by the range of the input variable resulting in a new range of just 1. mean normalization: involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://yyq.github.io/posts/2017/2017-08-02-machine-learning-week2/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2017-08-02T00:00:00+00:00">
<meta property="article:modified_time" content="2017-08-02T00:00:00+00:00">
<meta itemprop=name content="coursera Andrew Ng 机器学习第二周笔记 多个变量的线性回归">
<meta itemprop=description content="Linear Regression with Multiple Variables Multiple Features Gradient Descent for Multiple Variables Feature Scaling We can speed up gradient descent by having each of our input values in roughly the same range.
 feature scaling: involves dividing the input values by the range of the input variable resulting in a new range of just 1. mean normalization: involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero."><meta itemprop=datePublished content="2017-08-02T00:00:00+00:00">
<meta itemprop=dateModified content="2017-08-02T00:00:00+00:00">
<meta itemprop=wordCount content="283">
<meta itemprop=keywords content="ai,machine learning,Andrew Ng,"><meta name=twitter:card content="summary">
<meta name=twitter:title content="coursera Andrew Ng 机器学习第二周笔记 多个变量的线性回归">
<meta name=twitter:description content="Linear Regression with Multiple Variables Multiple Features Gradient Descent for Multiple Variables Feature Scaling We can speed up gradient descent by having each of our input values in roughly the same range.
 feature scaling: involves dividing the input values by the range of the input variable resulting in a new range of just 1. mean normalization: involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero.">
</head>
<body class="ma0 avenir bg-near-white">
<header>
<div class=bg-black>
<nav class="pv3 ph3 ph4-ns" role=navigation>
<div class="flex-l justify-between items-center center">
<a href=/ class="f3 fw2 hover-white no-underline white-90 dib">
Young Story
</a>
<div class="flex-l items-center">
<div class=ananke-socials>
</div>
</div>
</div>
</nav>
</div>
</header>
<main class=pb7 role=main>
<article class="flex-l flex-wrap justify-between mw8 center ph3">
<header class="mt4 w-100">
<aside class="instapaper_ignoref b helvetica tracked">
POSTS
</aside>
<div id=sharing class="mt3 ananke-socials">
</div>
<h1 class="f1 athelas mt3 mb1">coursera Andrew Ng 机器学习第二周笔记 多个变量的线性回归</h1>
<p class=tracked>
By <strong>
Young
</strong>
</p>
<time class="f6 mv4 dib tracked" datetime=2017-08-02T00:00:00Z>August 2, 2017</time>
</header>
<div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><h2 id=linear-regression-with-multiple-variables>Linear Regression with Multiple Variables</h2>
<h4 id=multiple-features>Multiple Features</h4>
<p><img src=./2017-08-02/multi-feature.png alt></p>
<p><img src=./2017-08-02/multi-feature-1.png alt></p>
<h4 id=gradient-descent-for-multiple-variables>Gradient Descent for Multiple Variables</h4>
<p><img src=./2017-08-02/gradient-for-m.png alt></p>
<p><img src=./2017-08-02/gradient-for-m-1.png alt></p>
<h4 id=feature-scaling>Feature Scaling</h4>
<p>We can speed up gradient descent by having each of our input values in roughly the same range.</p>
<ul>
<li><strong>feature scaling</strong>: involves dividing the input values by the range of the input variable resulting in a new range of just 1.</li>
<li><strong>mean normalization</strong>: involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero.</li>
</ul>
<p><img src=./2017-08-02/feature-scale.png alt></p>
<p>ui是平均值，si是最大-最小或者是标准差</p>
<h4 id=learning-rate>Learning Rate</h4>
<ul>
<li><strong>Debugging gradient descent</strong>: Make a plot with number of iterations on the x-axis. Now plot the cost function, J(θ) over the number of iterations of gradient descent. If J(θ) ever increases, then you probably need to decrease α.</li>
<li><strong>Automatic convergence test</strong>: Declare convergence if J(θ) decreases by less than E in one iteration, where E is some small value such as 10−3. However in practice it&rsquo;s difficult to choose this threshold value.</li>
</ul>
<p><img src=./2017-08-02/learning-rate.png alt></p>
<p><strong>If α is too small: slow convergence.</strong></p>
<p><strong>If α is too large: ￼may not decrease on every iteration and thus may not converge.</strong></p>
<h4 id=features-and-polynomial-regression>Features and Polynomial Regression</h4>
<p>We can <strong>change the behavior or curve</strong> of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).</p>
<p>One important thing to keep in mind is, if you choose your features this way then feature scaling becomes very important.</p>
<h4 id=normal-equation>Normal Equation</h4>
<p><img src=./2017-08-02/normal-equation.png alt></p>
<p><img src=./2017-08-02/normal-equation-1.png alt></p>
<p>if XtX is <strong>noninvertible</strong>, the common causes might be having :</p>
<ul>
<li>Redundant features, where two features are very closely related (i.e. they are linearly dependent)</li>
<li>Too many features (e.g. m ≤ n). In this case, delete some features or use &ldquo;regularization&rdquo; (to be explained in a later lesson).</li>
</ul>
<ul class=pa0>
<li class=list>
<a href=/tags/ai class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">ai</a>
</li>
<li class=list>
<a href=/tags/machine-learning class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">machine learning</a>
</li>
<li class=list>
<a href=/tags/andrew-ng class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Andrew Ng</a>
</li>
</ul>
<div class="mt6 instapaper_ignoref">
</div>
</div>
<aside class="w-30-l mt6-l">
<div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links">
<p class="f5 b mb3">Related</p>
<ul class="pa0 list">
<li class=mb2>
<a href=/posts/2017/2017-08-01-machine-learning-week1/>coursera Andrew Ng 机器学习第一周笔记 线性回归</a>
</li>
</ul>
</div>
</aside>
</article>
</main>
<footer class="bg-black bottom-0 w-100 pa3" role=contentinfo>
<div class="flex justify-between">
<a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://yyq.github.io/>
&copy; Young Story 2022
</a>
<div>
<div class=ananke-socials>
</div></div>
</div>
</footer>
</body>
</html>