<!doctype html><html lang=en-us>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge,chrome=1">
<title>kaggle | favorita-grocery-sales-forecasting | Young Story</title>
<meta name=viewport content="width=device-width,minimum-scale=1">
<meta name=description content="我的kaggle账号:https://www.kaggle.com/yyqing/competitions
输入数据为某连锁商店的各个店铺各个商品的销量，预测接下来16天的个店铺的各个商品的销量
在Public LB做到了3%，等privateLB出来之后，掉到了6%，发现提交历史记录里，自己的最好的单模型成绩比自己最后提交的组合模型答案成绩还好，可是最好的但模型成绩在publicLB都没有进入10%,心里慌啊，用尽全力的调整到3%啊。终于，自己对LB做到了overfit了，呵呵，教训啊！我的最好单模型成绩public LB 510, private LB 518, 组合模型成绩public LB 508 private LB 519.
1 Lessons learned of myself:  特征工程比模型调参更重要，重要性大出一个数量级 模型调参时，学习率先从较大的数字开始，节约时间 有关日期的比赛，本地cv日期的选择很重要，和最终测试日期有相似性才比较好 本地的每一次运行，cv，参数，分数都需要很好的记录下来，供后期对比分析 很多大神都开始在回归类题目里面开始用NN了，确实成绩会比xgb lgb等会有较大提升  2 Lessons learned from others: 总结两个观点先：
 特征工程特别重要 神经网络要胜过决策树boost了  每一段摘要都标记原作者，原文章标题，和原文链接
2.1 Eureka 1st place solution 借鉴了四个别人的模型，cnn，lstm，lgbm，lgbm
只用了2017年的数据，训练集，0531-0719 or 0614-0719, 不同的模型用不同的训练集，验证集，0726-0810
数据处理：把空值和负值都填0
特征工程：
 基本特征，分类（store,item,family,class,cluseter）,打折否，day_of_week(only for model 3); 统计特征，时间窗口，最近日期:[1,3,5,7,14,30,60,140]；等时间窗口[1] * 16, [7] * 20; 关键特征：store x item， item， store x class, target： promotion, unit_sales, zeros, 方法：mean,median,max,min,std, day since last appearance.">
<meta name=generator content="Hugo 0.91.2">
<meta name=ROBOTS content="NOINDEX, NOFOLLOW">
<link rel=stylesheet href=/ananke/css/main.min.css>
<meta property="og:title" content="kaggle | favorita-grocery-sales-forecasting">
<meta property="og:description" content="我的kaggle账号:https://www.kaggle.com/yyqing/competitions
输入数据为某连锁商店的各个店铺各个商品的销量，预测接下来16天的个店铺的各个商品的销量
在Public LB做到了3%，等privateLB出来之后，掉到了6%，发现提交历史记录里，自己的最好的单模型成绩比自己最后提交的组合模型答案成绩还好，可是最好的但模型成绩在publicLB都没有进入10%,心里慌啊，用尽全力的调整到3%啊。终于，自己对LB做到了overfit了，呵呵，教训啊！我的最好单模型成绩public LB 510, private LB 518, 组合模型成绩public LB 508 private LB 519.
1 Lessons learned of myself:  特征工程比模型调参更重要，重要性大出一个数量级 模型调参时，学习率先从较大的数字开始，节约时间 有关日期的比赛，本地cv日期的选择很重要，和最终测试日期有相似性才比较好 本地的每一次运行，cv，参数，分数都需要很好的记录下来，供后期对比分析 很多大神都开始在回归类题目里面开始用NN了，确实成绩会比xgb lgb等会有较大提升  2 Lessons learned from others: 总结两个观点先：
 特征工程特别重要 神经网络要胜过决策树boost了  每一段摘要都标记原作者，原文章标题，和原文链接
2.1 Eureka 1st place solution 借鉴了四个别人的模型，cnn，lstm，lgbm，lgbm
只用了2017年的数据，训练集，0531-0719 or 0614-0719, 不同的模型用不同的训练集，验证集，0726-0810
数据处理：把空值和负值都填0
特征工程：
 基本特征，分类（store,item,family,class,cluseter）,打折否，day_of_week(only for model 3); 统计特征，时间窗口，最近日期:[1,3,5,7,14,30,60,140]；等时间窗口[1] * 16, [7] * 20; 关键特征：store x item， item， store x class, target： promotion, unit_sales, zeros, 方法：mean,median,max,min,std, day since last appearance.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://yyq.github.io/posts/2018/2018-01-17-favorita-grocery-sales-forecasting/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2018-01-17T00:00:00+00:00">
<meta property="article:modified_time" content="2018-01-17T00:00:00+00:00">
<meta itemprop=name content="kaggle | favorita-grocery-sales-forecasting">
<meta itemprop=description content="我的kaggle账号:https://www.kaggle.com/yyqing/competitions
输入数据为某连锁商店的各个店铺各个商品的销量，预测接下来16天的个店铺的各个商品的销量
在Public LB做到了3%，等privateLB出来之后，掉到了6%，发现提交历史记录里，自己的最好的单模型成绩比自己最后提交的组合模型答案成绩还好，可是最好的但模型成绩在publicLB都没有进入10%,心里慌啊，用尽全力的调整到3%啊。终于，自己对LB做到了overfit了，呵呵，教训啊！我的最好单模型成绩public LB 510, private LB 518, 组合模型成绩public LB 508 private LB 519.
1 Lessons learned of myself:  特征工程比模型调参更重要，重要性大出一个数量级 模型调参时，学习率先从较大的数字开始，节约时间 有关日期的比赛，本地cv日期的选择很重要，和最终测试日期有相似性才比较好 本地的每一次运行，cv，参数，分数都需要很好的记录下来，供后期对比分析 很多大神都开始在回归类题目里面开始用NN了，确实成绩会比xgb lgb等会有较大提升  2 Lessons learned from others: 总结两个观点先：
 特征工程特别重要 神经网络要胜过决策树boost了  每一段摘要都标记原作者，原文章标题，和原文链接
2.1 Eureka 1st place solution 借鉴了四个别人的模型，cnn，lstm，lgbm，lgbm
只用了2017年的数据，训练集，0531-0719 or 0614-0719, 不同的模型用不同的训练集，验证集，0726-0810
数据处理：把空值和负值都填0
特征工程：
 基本特征，分类（store,item,family,class,cluseter）,打折否，day_of_week(only for model 3); 统计特征，时间窗口，最近日期:[1,3,5,7,14,30,60,140]；等时间窗口[1] * 16, [7] * 20; 关键特征：store x item， item， store x class, target： promotion, unit_sales, zeros, 方法：mean,median,max,min,std, day since last appearance."><meta itemprop=datePublished content="2018-01-17T00:00:00+00:00">
<meta itemprop=dateModified content="2018-01-17T00:00:00+00:00">
<meta itemprop=wordCount content="804">
<meta itemprop=keywords content="kaggle,machine learning,regression,"><meta name=twitter:card content="summary">
<meta name=twitter:title content="kaggle | favorita-grocery-sales-forecasting">
<meta name=twitter:description content="我的kaggle账号:https://www.kaggle.com/yyqing/competitions
输入数据为某连锁商店的各个店铺各个商品的销量，预测接下来16天的个店铺的各个商品的销量
在Public LB做到了3%，等privateLB出来之后，掉到了6%，发现提交历史记录里，自己的最好的单模型成绩比自己最后提交的组合模型答案成绩还好，可是最好的但模型成绩在publicLB都没有进入10%,心里慌啊，用尽全力的调整到3%啊。终于，自己对LB做到了overfit了，呵呵，教训啊！我的最好单模型成绩public LB 510, private LB 518, 组合模型成绩public LB 508 private LB 519.
1 Lessons learned of myself:  特征工程比模型调参更重要，重要性大出一个数量级 模型调参时，学习率先从较大的数字开始，节约时间 有关日期的比赛，本地cv日期的选择很重要，和最终测试日期有相似性才比较好 本地的每一次运行，cv，参数，分数都需要很好的记录下来，供后期对比分析 很多大神都开始在回归类题目里面开始用NN了，确实成绩会比xgb lgb等会有较大提升  2 Lessons learned from others: 总结两个观点先：
 特征工程特别重要 神经网络要胜过决策树boost了  每一段摘要都标记原作者，原文章标题，和原文链接
2.1 Eureka 1st place solution 借鉴了四个别人的模型，cnn，lstm，lgbm，lgbm
只用了2017年的数据，训练集，0531-0719 or 0614-0719, 不同的模型用不同的训练集，验证集，0726-0810
数据处理：把空值和负值都填0
特征工程：
 基本特征，分类（store,item,family,class,cluseter）,打折否，day_of_week(only for model 3); 统计特征，时间窗口，最近日期:[1,3,5,7,14,30,60,140]；等时间窗口[1] * 16, [7] * 20; 关键特征：store x item， item， store x class, target： promotion, unit_sales, zeros, 方法：mean,median,max,min,std, day since last appearance.">
</head>
<body class="ma0 avenir bg-near-white">
<header>
<div class=bg-black>
<nav class="pv3 ph3 ph4-ns" role=navigation>
<div class="flex-l justify-between items-center center">
<a href=/ class="f3 fw2 hover-white no-underline white-90 dib">
Young Story
</a>
<div class="flex-l items-center">
<div class=ananke-socials>
</div>
</div>
</div>
</nav>
</div>
</header>
<main class=pb7 role=main>
<article class="flex-l flex-wrap justify-between mw8 center ph3">
<header class="mt4 w-100">
<aside class="instapaper_ignoref b helvetica tracked">
POSTS
</aside>
<div id=sharing class="mt3 ananke-socials">
</div>
<h1 class="f1 athelas mt3 mb1">kaggle | favorita-grocery-sales-forecasting</h1>
<p class=tracked>
By <strong>
Young
</strong>
</p>
<time class="f6 mv4 dib tracked" datetime=2018-01-17T00:00:00Z>January 17, 2018</time>
</header>
<div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><p>我的kaggle账号:<a href=https://www.kaggle.com/yyqing/competitions>https://www.kaggle.com/yyqing/competitions</a></p>
<p>输入数据为某连锁商店的各个店铺各个商品的销量，预测接下来16天的个店铺的各个商品的销量</p>
<p>在Public LB做到了3%，等privateLB出来之后，掉到了6%，发现提交历史记录里，自己的最好的单模型成绩比自己最后提交的组合模型答案成绩还好，可是最好的但模型成绩在publicLB都没有进入10%,心里慌啊，用尽全力的调整到3%啊。终于，自己对LB做到了overfit了，呵呵，教训啊！我的最好单模型成绩public LB 510, private LB 518, 组合模型成绩public LB 508 private LB 519.</p>
<h2 id=1-lessons-learned-of-myself>1 Lessons learned of myself:</h2>
<ol>
<li>特征工程比模型调参更重要，重要性大出一个数量级</li>
<li>模型调参时，学习率先从较大的数字开始，节约时间</li>
<li>有关日期的比赛，本地cv日期的选择很重要，和最终测试日期有相似性才比较好</li>
<li>本地的每一次运行，cv，参数，分数都需要很好的记录下来，供后期对比分析</li>
<li>很多大神都开始在回归类题目里面开始用NN了，确实成绩会比xgb lgb等会有较大提升</li>
</ol>
<h2 id=2-lessons-learned-from-others>2 Lessons learned from others:</h2>
<p>总结两个观点先：</p>
<ol>
<li>特征工程特别重要</li>
<li>神经网络要胜过决策树boost了</li>
</ol>
<p>每一段摘要都标记原作者，原文章标题，和原文链接</p>
<h3 id=21-eureka-1st-place-solutionhttpswwwkagglecomcfavorita-grocery-sales-forecastingdiscussion47582>2.1 <a href=https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47582>Eureka 1st place solution</a></h3>
<p>借鉴了四个别人的模型，cnn，lstm，lgbm，lgbm</p>
<p>只用了2017年的数据，训练集，0531-0719 or 0614-0719, 不同的模型用不同的训练集，验证集，0726-0810</p>
<p>数据处理：把空值和负值都填0</p>
<p>特征工程：</p>
<ol>
<li>基本特征，分类（store,item,family,class,cluseter）,打折否，day_of_week(only for model 3);</li>
<li>统计特征，时间窗口，最近日期:[1,3,5,7,14,30,60,140]；等时间窗口[1] * 16, [7] * 20; <strong>关键特征：store x item， item， store x class</strong>, target： promotion, unit_sales, zeros, 方法：mean,median,max,min,std, day since last appearance. difference of mean value between adjacent time windows(only for equal time windows)</li>
<li>无用的特征，节假日， 其他的keys例如，cluster x item, store x family</li>
</ol>
<p>单模型：</p>
<ol>
<li>model_1 : 0.506 / 0.511 , 16 lgb models trained for each day <a href=https://www.kaggle.com/shixw125/1st-place-lgb-model-public-0-506-private-0-511>code</a></li>
<li>model_2 : 0.507 / 0.513 , 16 nn models trained for each day <a href=https://www.kaggle.com/shixw125/1st-place-nn-model-public-0-507-private-0-513>code</a></li>
<li>model_3 : 0.512 / 0.515，1 lgb model for 16 days with almost same features as model_1</li>
<li>0.517 / 0.519，1 nn model based on @sjv&rsquo;s code</li>
</ol>
<p>组合模型：
Stacking doesn&rsquo;t work well this time, our best model is linear blend of 4 single models.
final submission = 0.42*model_1 + 0.28 * model_2 + 0.18 * model_3 + 0.12 * model_4
public = 0.504 , private = 0.509</p>
<h3 id=22-luck-yu-2nd-place-solution-overviewhttpswwwkagglecomcfavorita-grocery-sales-forecastingdiscussion47568>2.2 <a href=https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47568>Luck Yu 2nd place solution overview</a></h3>
<p>他上来先感谢了sjv CPMP。</p>
<p>用的wavenet</p>
<p>Feeding data with mini-batch with randomly sampled 128 sequence. Then randomly to choose the start of decode/target date. So, we could say somehow the model will see different data for each training iteration. because the total dataset is around 170000 (seq) x 365 days。这么训练下来，wavenet应该能够处理好过拟合</p>
<p>用最后16天来做验证集</p>
<h3 id=23-slonoslon-3rd-place-solution-overviewhttpswwwkagglecomcfavorita-grocery-sales-forecastingdiscussion47560>2.3 <a href=https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47560>slonoslon 3rd place solution overview</a></h3>
<p>lgbm + nn. 遇到时间序列问题的时候，两个最重要的事情是validation和bagging。</p>
<p>训练集, I took a history of 80 consequtive sales days for each (item, store pair), and used the next 16 sales days as a train set target.</p>
<p>about bagging, our final result includes models, trained on 10 runs each. 直接平均。</p>
<p>另外一个部分的bagging，what the FUCK ? After each training epoch (including the very first) I predicted the targets, and then averaged these predictions. I&rsquo;ve read about averaging a few last epochs, but never heard about averaging everything, even the first very underfitted epoch. Yet my validation consistently showed that it does improve the results.</p>
<p>组合方案，组合了CNN LSTM GRU，直接平均。All of them have 3 top (convolutional or recurrent) layers, 2 or 3 bottom fully-connected layers and embedding layers for categorical features</p>
<h3 id=24-sjv-4th-place-solution-overviewhttpswwwkagglecomcfavorita-grocery-sales-forecastingdiscussion47529>2.4 <a href=https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47529>sjv 4th-Place Solution Overview</a>:</h3>
<p>只分享他的单模型取得.499成绩</p>
<p>模型：双向LSTM，他的源代码<a href=https://github.com/sjvasquez/web-traffic-forecasting>code</a></p>
<p>input representation：原始的时间序列的值，分类变量，手动做的特征（lags,diffs,rolling statistics, date features, conditioning time series, average sales for a given product/store/etc.）</p>
<p>validation: 用的是过去365天随机的，这么做的目的是为了防止过拟合于某一个validation set/weekly/monthly trends.</p>
<p>各路大神都吐槽了这个题目给的数据集的表示方式的错误。包括user ranking总榜第一的人也吐槽了。</p>
<h3 id=25-lingzhi-5th-place-solutionhttpswwwkagglecomcfavorita-grocery-sales-forecastingdiscussion47556>2.5 <a href=https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47556>Lingzhi 5th Place Solution</a></h3>
<p>用了3个模型，lgbm，cnn+dnn, seq2seq RNN. 直接平均。每个模型都用不同的随机种子训练多次，求平均值。每个单模型可以做到在private LB里1%（这个就比较屌了）。</p>
<p>lgbm，公开kernel的的升级版本，加了更多的特征，数据，日期。</p>
<p>cnn+dnn, This is a traditional NN model, where the CNN part is a dilated causal convolution inspired by WaveNet, and the DNN part is 2 FC layers connected to raw sales sequences. Then the inputs are concatenated together with categorical embeddings and future promotions, and directly output to 16 future days of predictions.</p>
<p>RNN, This is a seq2seq model with a similar architecture of @Arthur Suilin&rsquo;s solution for the web traffic prediction. Encoder and decoder are both GRUs. The hidden states of the encoder are passed to the decoder through an FC layer connector. This is useful to improve the accuracy significantly.</p>
<p>特征工，对lgb来说，加了mean sales，count of promotions, count of zeros. 在nn中，item mean和year-ago和quarter-ago sales. categorical featues and time features(weekday, day of month) are fed as embeddings.</p>
<p>据说这个帖子给他帮助很大，<a href=https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/43795>web traffic prediction</a>这个很有用，比较通用的时间序列解决方案。</p>
<h3 id=26-nicolas-6th-place-solution-overviewhttpswwwkagglecomcfavorita-grocery-sales-forecastingdiscussion47575>2.6 <a href=https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47575>Nicolas 6th Place Solution Overview</a></h3>
<p>组合了两个可以 517 private的单模型， 基本思路为：lgbm使用2017的数据，超过500个特征，相对可靠的cv on 0726-0810</p>
<p>他把他所有做的特征按照重要顺序都排列出来了，这个厉害了，我就直接引用吧</p>
<ul>
<li>ma_median * isd_avg / isd_week_avg - popularized early on in the competition.</li>
<li>ma_median - Coupled with binary for whether or not it is equal to 0.</li>
<li>Day-of-week averages - Different time periods including 7, 14, 28, 56, 112.</li>
<li>Days since appeared - Difference between the &lsquo;start date&rsquo; of the training cycle and the first date the item showed up in the original train file.</li>
<li>Quantiles for several different time-spans.</li>
<li>Whether the item will be onpromotion.</li>
<li>Simple averages for different time-spans.</li>
<li>Item-cluster means - The past 5-day mean was a good predictor.</li>
<li>Future promotional &lsquo;sums&rsquo; - E.g: sum of onpromotion 8/16 through 8/18.</li>
<li>mean_no_zero_sales - Mean of instances where unit_sales > 0.</li>
<li>Frequency encoding - calculate the frequency of appearance for items, stores, families and classes (four columns that each sum to 1).</li>
</ul>
<p>上面这些特征都是能够直接提高成绩的重要predictors，下面的这些特征是从整体上提升</p>
<ul>
<li>One-hot-encoding clusters, states, types, cities and families.</li>
<li>Weekend and weekday means.</li>
<li>Staggered mean data (e.g. 8/07 &ndash;> 8/14 for the test-set).</li>
<li>Staggered quantile data.</li>
<li>Past number of zero sales for different time-spans.</li>
<li>Past number of promotional days.</li>
<li>Past promotional sales averages.</li>
<li>Past day-of-week quantile data.</li>
</ul>
<p>他的队友也是使用的lgbm，不过都使用了不同的特征。</p>
<h3 id=27-cpmp-8th-solutionhttpswwwkagglecomcfavorita-grocery-sales-forecastingdiscussion47564>2.7 <a href=https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47564>CPMP 8th solution</a></h3>
<p>single NN got 508 on public and 515 on private. single lgb got 514 on private.</p>
<p>缺失值的处理：用之后的数据直接填补之前的空白。</p>
<p>seasonality，时间序列处理中，两个关键点是时期性和cv设定。这个题目中weekly时期影响很强而yearly很小</p>
<p>cv setting,采用了两段时间2017-7-15，2017-7-31 和 2017-8-1，2017-8-15</p>
<p>特征工程，log1p，mean,max，proportion of zero entries,过去7天的每个单天销量，过去八个礼拜的相同weekday的均值，对于每个日期列出过去364天的销量和促销状态，产品的类别，商店</p>
<p>神经网络模型，3个dense层 relu激活。class和store被编码成4个维度的向量，一次可以预测16个了，最后卷积一个onpromotion向量</p>
<p>其他模型：lgb做到了510 on public lb and 517 on private lb</p>
<p>Giba大佬评论说了：<strong>I always say and continue saying, don&rsquo;t trust public LB. Specially in a time series challenge where public LB is calculated using 5 days ahead and private is from 6 to 16 days ahead, they are completely different models.</strong></p>
<ul class=pa0>
<li class=list>
<a href=/tags/kaggle class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">kaggle</a>
</li>
<li class=list>
<a href=/tags/machine-learning class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">machine learning</a>
</li>
<li class=list>
<a href=/tags/regression class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">regression</a>
</li>
</ul>
<div class="mt6 instapaper_ignoref">
</div>
</div>
<aside class="w-30-l mt6-l">
<div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links">
<p class="f5 b mb3">Related</p>
<ul class="pa0 list">
<li class=mb2>
<a href=/posts/2017/2017-11-29-kaggle/>第一次kaggle参赛，铜牌一枚</a>
</li>
<li class=mb2>
<a href=/posts/2017/2017-10-23-xgboost-tune/>xgboost模型调试攻略</a>
</li>
<li class=mb2>
<a href=/posts/2017/2017-10-02-machine-learning-ex-progress/>17年9月份机器学习知识进展</a>
</li>
<li class=mb2>
<a href=/posts/2017/2017-08-12-machine-learning-exercise/>coursera Andrew Ng 机器学习 编程习题总结</a>
</li>
<li class=mb2>
<a href=/posts/2017/2017-08-11-machine-learning-week11/>coursera Andrew Ng 机器学习第十一周笔记 应用举例：photo OCR</a>
</li>
<li class=mb2>
<a href=/posts/2017/2017-08-10-machine-learning-week10/>coursera Andrew Ng 机器学习第十周笔记 大数据量的机器学习</a>
</li>
<li class=mb2>
<a href=/posts/2017/2017-08-09-machine-learning-week9/>coursera Andrew Ng 机器学习第九周笔记 异常检测与推荐系统</a>
</li>
<li class=mb2>
<a href=/posts/2017/2017-08-08-machine-learning-week8/>coursera Andrew Ng 机器学习第八周笔记 降低维度</a>
</li>
<li class=mb2>
<a href=/posts/2017/2017-08-07-machine-learning-week7/>coursera Andrew Ng 机器学习第七周笔记 支持向量机</a>
</li>
<li class=mb2>
<a href=/posts/2017/2017-08-06-machine-learning-week6/>coursera Andrew Ng 机器学习第六周笔记 系统设计与优化技巧</a>
</li>
<li class=mb2>
<a href=/posts/2017/2017-08-05-machine-learning-week5/>coursera Andrew Ng 机器学习第五周笔记 神经网络的学习过程</a>
</li>
<li class=mb2>
<a href=/posts/2017/2017-08-04-machine-learning-week4/>coursera Andrew Ng 机器学习第四周笔记 神经网络模型</a>
</li>
<li class=mb2>
<a href=/posts/2017/2017-08-03-machine-learning-week3/>coursera Andrew Ng 机器学习第三周笔记 逻辑回归与泛化</a>
</li>
<li class=mb2>
<a href=/posts/2017/2017-08-02-machine-learning-week2/>coursera Andrew Ng 机器学习第二周笔记 多个变量的线性回归</a>
</li>
<li class=mb2>
<a href=/posts/2017/2017-08-01-machine-learning-week1/>coursera Andrew Ng 机器学习第一周笔记 线性回归</a>
</li>
</ul>
</div>
</aside>
</article>
</main>
<footer class="bg-black bottom-0 w-100 pa3" role=contentinfo>
<div class="flex justify-between">
<a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://yyq.github.io/>
&copy; Young Story 2022
</a>
<div>
<div class=ananke-socials>
</div></div>
</div>
</footer>
</body>
</html>